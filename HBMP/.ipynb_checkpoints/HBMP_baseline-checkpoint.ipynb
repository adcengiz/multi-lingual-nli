{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py imports\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from classifier import NLIModel\n",
    "from corpora import MultiNLI, SciTail, StanfordNLI, AllNLI, BreakingNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name):\n",
    "    try:\n",
    "        os.makedirs(name)\n",
    "    except OSError as ex:\n",
    "        if ex.errno == errno.EEXIST and os.path.isdir(name):\n",
    "            # ignore existing directory\n",
    "            pass\n",
    "        else:\n",
    "            # a different error happened\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parser done.\n",
      "config done.\n",
      "tokenizing inputs.\n",
      "getting labels.\n",
      "corpus config started.\n"
     ]
    }
   ],
   "source": [
    "# %tb\n",
    "def main():\n",
    "    \n",
    "    parser = ArgumentParser(description='Helsinki NLI')\n",
    "    parser.add_argument(\"--corpus\",\n",
    "                        type=str,\n",
    "                        choices=('snli', 'breaking_nli', 'multinli_matched', 'multinli_mismatched', 'scitail', 'all_nli'),\n",
    "                        default='snli')\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=20)\n",
    "    parser.add_argument('--batch_size',\n",
    "                        type=int,\n",
    "                        default=64)\n",
    "    parser.add_argument(\"--encoder_type\",\n",
    "                        type=str,\n",
    "                        choices=('BiLSTMMaxPoolEncoder',\n",
    "                                 'LSTMEncoder',\n",
    "                                 'HBMP'),\n",
    "                        default='HBMP')\n",
    "    parser.add_argument(\"--activation\",\n",
    "                        type=str,\n",
    "                        choices=('tanh', 'relu', 'leakyrelu'),\n",
    "                        default='relu')\n",
    "    parser.add_argument(\"--optimizer\",\n",
    "                        type=str,\n",
    "                        choices=('rprop',\n",
    "                                 'adadelta',\n",
    "                                 'adagrad',\n",
    "                                 'rmsprop',\n",
    "                                 'adamax',\n",
    "                                 'asgd',\n",
    "                                 'adam',\n",
    "                                 'sgd'),\n",
    "                        default='adam')\n",
    "    parser.add_argument('--embed_dim',\n",
    "                        type=int,\n",
    "                        default=300)\n",
    "    parser.add_argument('--fc_dim',\n",
    "                        type=int,\n",
    "                        default=600)\n",
    "    parser.add_argument('--hidden_dim',\n",
    "                        type=int,\n",
    "                        default=600)\n",
    "    parser.add_argument('--layers',\n",
    "                        type=int,\n",
    "                        default=1)\n",
    "    parser.add_argument('--dropout',\n",
    "                        type=float,\n",
    "                        default=0.1)\n",
    "    parser.add_argument('--learning_rate',\n",
    "                        type=float,\n",
    "                        default=0.0005)\n",
    "    parser.add_argument('--lr_patience',\n",
    "                        type=int,\n",
    "                        default=1)\n",
    "    parser.add_argument('--lr_decay',\n",
    "                        type=float,\n",
    "                        default=0.99)\n",
    "    parser.add_argument('--lr_reduction_factor',\n",
    "                        type=float,\n",
    "                        default=0.2)\n",
    "    parser.add_argument('--weight_decay',\n",
    "                        type=float,\n",
    "                        default=0)\n",
    "    parser.add_argument('--gpu',\n",
    "                        type=int,\n",
    "                        default=0)\n",
    "    parser.add_argument('--preserve_case',\n",
    "                        action='store_false',\n",
    "                        dest='lower')\n",
    "    parser.add_argument('--word_embedding',\n",
    "                        type=str,\n",
    "                        default='glove.840B.300d')\n",
    "    parser.add_argument('--resume_snapshot',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    parser.add_argument('--early_stopping_patience',\n",
    "                        type=int,\n",
    "                        default=3)\n",
    "    parser.add_argument('--save_path',\n",
    "                        type=str,\n",
    "                        default='results')\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=1234)\n",
    "    \n",
    "    print (\"parser done.\")\n",
    "#     config = parser.parse_args()\n",
    "    config, unknown = parser.parse_known_args()\n",
    "    print (\"config done.\")\n",
    "    \n",
    "    np.random.seed(config.seed)\n",
    "    torch.manual_seed(config.seed)\n",
    "    torch.cuda.manual_seed(config.seed)\n",
    "    random.seed(config.seed)\n",
    "\n",
    "    torch.cuda.device(config.gpu)\n",
    "\n",
    "    print (\"tokenizing inputs.\")\n",
    "    inputs = data.Field(lower=config.lower, tokenize='spacy')\n",
    "    print (\"getting labels.\")\n",
    "    labels = data.Field(sequential=False, unk_token=None)\n",
    "    category_field = data.Field(sequential=False)\n",
    "    id_field = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "    print (\"corpus config started.\")\n",
    "    if config.corpus == 'multinli_matched':\n",
    "        train, dev, test = MultiNLI.splits_matched(inputs, labels, id_field)\n",
    "        id_field.build_vocab(train, dev, test)\n",
    "    elif config.corpus == 'multinli_mismatched':\n",
    "        train, dev, test = MultiNLI.splits_mismatched(inputs, labels, id_field)\n",
    "        id_field.build_vocab(train, dev, test)\n",
    "    elif config.corpus == 'scitail':\n",
    "        train, dev, test = SciTail.splits(inputs, labels)\n",
    "    elif config.corpus == 'all_nli':\n",
    "        train, dev, test = AllNLI.splits(inputs, labels, id_field)\n",
    "        id_field.build_vocab(train, dev, test)\n",
    "    elif config.corpus == 'breaking_nli':\n",
    "        train, dev, test = BreakingNLI.splits(inputs, labels, category_field)\n",
    "        category_field.build_vocab(test)\n",
    "    else:\n",
    "        train, dev, test = StanfordNLI.splits(inputs, labels)\n",
    "\n",
    "    print (\"Building vocab.\")\n",
    "    inputs.build_vocab(train, dev, test)\n",
    "    labels.build_vocab(train)\n",
    "    \n",
    "    print (\"Embedding configuration started.\")\n",
    "    if config.word_embedding:\n",
    "        pretrained_embedding = os.path.join(os.getcwd(), '.vector_cache/'+config.corpus+'_'+config.word_embedding+'.pt')\n",
    "        if os.path.isfile(pretrained_embedding):\n",
    "            inputs.vocab.vectors = torch.load(pretrained_embedding,\n",
    "                           map_location=lambda storage, location: storage.cuda(config.gpu))\n",
    "        else:\n",
    "            print('Downloading pretrained {} word embeddings\\n'.format(config.word_embedding))\n",
    "            inputs.vocab.load_vectors(config.word_embedding)\n",
    "            make_dirs(os.path.dirname(pretrained_embedding))\n",
    "            torch.save(inputs.vocab.vectors, pretrained_embedding)\n",
    "\n",
    "    print (\"Building iterators.\")\n",
    "    train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test),\n",
    "                                                                 batch_size=config.batch_size,\n",
    "                                                                 device=config.gpu)\n",
    "    print (\"Iterators ready.\")\n",
    "    config.embed_size = len(inputs.vocab)\n",
    "    config.out_dim = len(labels.vocab)\n",
    "    config.cells = config.layers\n",
    "\n",
    "    if config.encoder_type != 'LSTMEncoder':\n",
    "        config.cells *= 2\n",
    "\n",
    "    if config.resume_snapshot:\n",
    "        model = torch.load(config.resume_snapshot,\n",
    "                           map_location=lambda storage, location: storage.cuda(config.gpu))\n",
    "    else:\n",
    "        model = NLIModel(config)\n",
    "        if config.word_embedding:\n",
    "            model.sentence_embedding.word_embedding.weight.data = inputs.vocab.vectors\n",
    "            model.cuda(device=config.gpu)\n",
    "\n",
    "    # Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    if config.optimizer == 'adadelta':\n",
    "        optim_algorithm = optim.Adadelta\n",
    "    elif config.optimizer == 'adagrad':\n",
    "        optim_algorithm = optim.Adagrad\n",
    "    elif config.optimizer == 'adam':\n",
    "        optim_algorithm = optim.Adam\n",
    "    elif config.optimizer == 'adamax':\n",
    "        optim_algorithm = optim.Adamax\n",
    "    elif config.optimizer == 'asgd':\n",
    "        optim_algorithm = optim.ASGD\n",
    "    elif config.optimizer == 'rmsprop':\n",
    "        optim_algorithm = optim.RMSprop\n",
    "    elif config.optimizer == 'rprop':\n",
    "        optim_algorithm = optim.Rprop\n",
    "    elif config.optimizer == 'sgd':\n",
    "        optim_algorithm = optim.SGD\n",
    "    else:\n",
    "        raise Exception('Unknown optimization optimizer: \"%s\"' % config.optimizer)\n",
    "\n",
    "    optimizer = optim_algorithm(model.parameters(),\n",
    "                                lr=config.learning_rate,\n",
    "                                weight_decay=config.weight_decay)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                               'min',\n",
    "                                               factor=config.lr_reduction_factor,\n",
    "                                               patience=config.lr_patience,\n",
    "                                               verbose=False,\n",
    "                                               min_lr=1e-5)\n",
    "\n",
    "    iterations = 0\n",
    "    best_dev_acc = -1\n",
    "    dev_accuracies = []\n",
    "    best_dev_loss = 1\n",
    "    early_stopping = 0\n",
    "    stop_training = False\n",
    "    train_iter.repeat = False\n",
    "    make_dirs(config.save_path)\n",
    "\n",
    "    # Print parameters and config\n",
    "    print('\\nConfig: {}\\n'.format(sys.argv[1:]))\n",
    "    print(config)\n",
    "\n",
    "    # Print the model\n",
    "    print('Model:\\n')\n",
    "    print(model)\n",
    "    print('\\n')\n",
    "    params = sum([p.numel() for p in model.parameters()])\n",
    "    print('Parameters: {}'.format(params))\n",
    "    print('\\nTraining started...\\n')\n",
    "\n",
    "    # Train for the number of epochs specified\n",
    "    for epoch in range(config.epochs):\n",
    "        if stop_training == True:\n",
    "            break\n",
    "\n",
    "        train_iter.init_epoch()\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        all_losses = []\n",
    "        train_accuracies = []\n",
    "        all_losses = []\n",
    "\n",
    "        optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * config.lr_decay if epoch>0\\\n",
    "        and config.optimizer == 'sgd' else optimizer.param_groups[0]['lr']\n",
    "        print('\\nEpoch: {:>02.0f}/{:<02.0f}'.format(epoch+1, config.epochs), end=' ')\n",
    "        print('(Learning rate: {})'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            iterations += 1\n",
    "\n",
    "            answer = model(batch)\n",
    "            # sys.exit()\n",
    "            # Calculate accuracy\n",
    "            n_correct += (torch.max(answer, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "            n_total += batch.batch_size\n",
    "            train_acc = 100. * n_correct/n_total\n",
    "            train_accuracies.append(train_acc)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(answer, batch.label)\n",
    "            all_losses.append(loss.data[0])\n",
    "\n",
    "            # Backpropagate and update the learning rate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print('Progress: {:3.0f}% - Batch: {:>4.0f}/{:<4.0f} - Loss: {:6.2f}% - Accuracy: {:6.2f}%'.format(\n",
    "                100. * (1+batch_idx) / len(train_iter),\n",
    "                1+batch_idx, len(train_iter),\n",
    "                round(100. * np.mean(all_losses), 2),\n",
    "                round(np.mean(train_accuracies), 2)), end='\\r')\n",
    "\n",
    "            # Evaluate performance\n",
    "            # if iterations % config.dev_every == 0:\n",
    "            if 1+batch_idx == len(train_iter):\n",
    "                # Switch model to evaluation mode\n",
    "                model.eval()\n",
    "                dev_iter.init_epoch()\n",
    "\n",
    "                # Calculate Accuracy\n",
    "                n_dev_correct = 0\n",
    "                dev_loss = 0\n",
    "                dev_losses = []\n",
    "\n",
    "                for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "                    answer = model(dev_batch)\n",
    "                    n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == \\\n",
    "                        dev_batch.label.data).sum()\n",
    "                    dev_loss = criterion(answer, dev_batch.label)\n",
    "                    dev_losses.append(dev_loss.data[0])\n",
    "\n",
    "                dev_acc = 100. * n_dev_correct / len(dev)\n",
    "                dev_accuracies.append(dev_acc)\n",
    "\n",
    "                print('\\nDev loss: {}% - Dev accuracy: {}%'.format(round(100.*np.mean(dev_losses), 2), round(dev_acc, 2)))\n",
    "\n",
    "                # Update validation best accuracy if it is better than\n",
    "                # already stored\n",
    "                if dev_acc > best_dev_acc:\n",
    "\n",
    "                    best_dev_acc = dev_acc\n",
    "                    best_dev_epoch = 1+epoch\n",
    "                    snapshot_prefix = os.path.join(config.save_path, 'best')\n",
    "                    dev_snapshot_path = snapshot_prefix + \\\n",
    "                        '_{}_{}D_devacc_{}_epoch_{}.pt'.format(config.encoder_type, config.hidden_dim, round(dev_acc, 2), 1+epoch)\n",
    "\n",
    "                    # save model, delete previous snapshot\n",
    "                    torch.save(model, dev_snapshot_path)\n",
    "                    for f in glob.glob(snapshot_prefix + '*'):\n",
    "                        if f != dev_snapshot_path:\n",
    "                            os.remove(f)\n",
    "\n",
    "                # Check for early stopping\n",
    "                if np.mean(dev_losses) < best_dev_loss:\n",
    "                    best_dev_loss = np.mean(dev_losses)\n",
    "                else:\n",
    "                    early_stopping += 1\n",
    "\n",
    "                if early_stopping > config.early_stopping_patience and config.optimizer != 'sgd':\n",
    "                    stop_training = True\n",
    "                    print('\\nEarly stopping')\n",
    "\n",
    "                if config.optimizer == 'sgd' and optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "                    stop_training = True\n",
    "                    print('\\nEarly stopping')\n",
    "\n",
    "                # Update learning rate\n",
    "                scheduler.step(round(np.mean(dev_losses), 2))\n",
    "                dev_losses = []\n",
    "\n",
    "\n",
    "            # If training has completed, calculate the test scores\n",
    "            if stop_training == True or (1+epoch == config.epochs and 1+batch_idx == len(train_iter)):\n",
    "                print('\\nTraining completed after {} epocs.\\n'.format(1+epoch))\n",
    "\n",
    "\n",
    "                #Save the final model\n",
    "                final_snapshot_prefix = os.path.join(config.save_path, 'final')\n",
    "                final_snapshot_path = final_snapshot_prefix + \\\n",
    "                '_{}_{}D.pt'.format(config.encoder_type, config.hidden_dim)\n",
    "                torch.save(model, final_snapshot_path)\n",
    "                for f in glob.glob(final_snapshot_prefix + '*'):\n",
    "                    if f != final_snapshot_path:\n",
    "                        os.remove(f)\n",
    "\n",
    "                # Evaluate the best dev model\n",
    "                test_model = torch.load(dev_snapshot_path)\n",
    "                # Switch model to evaluation mode\n",
    "                test_model.eval()\n",
    "                test_iter.init_epoch()\n",
    "\n",
    "                # Calculate Accuracy\n",
    "                n_test_correct = 0\n",
    "                test_loss = 0\n",
    "                test_losses = []\n",
    "\n",
    "                for test_batch_idx, test_batch in enumerate(test_iter):\n",
    "                    answer = test_model(test_batch)\n",
    "                    n_test_correct += (torch.max(answer, 1)[1].view(test_batch.label.size()).data == \\\n",
    "                        test_batch.label.data).sum()\n",
    "                    test_loss = criterion(answer, test_batch.label)\n",
    "                    test_losses.append(test_loss.data[0])\n",
    "\n",
    "                test_acc = 100. * n_test_correct / len(test)\n",
    "\n",
    "                print('SUMMARY:')\n",
    "                print('Encoder: {}'.format(config.encoder_type))\n",
    "                if config.encoder_type == 'BiLSTMMaxPoolEncoder' or config.encoder_type == \\\n",
    "                'HBMP' or config.encoder_type == 'HAttentionBiLSTMEncoder':\n",
    "                    print('Sentence embedding size: {}D'.format(2*config.hidden_dim))\n",
    "                else:\n",
    "                    print('Sentence embedding size: {}D'.format(config.hidden_dim))\n",
    "\n",
    "                print('\\nMean dev accuracy: {:6.2f}%\\n'.format(round(np.mean(dev_accuracies)), 2))\n",
    "                print('BEST MODEL:')\n",
    "                print('Early stopping patience: {}'.format(config.early_stopping_patience))\n",
    "                print('Epoch: {}'.format(best_dev_epoch))\n",
    "                print('Dev accuracy: {:<6.2f}%'.format(round(best_dev_acc, 2)))\n",
    "                print('Test loss: {:<.2f}%'.format(round(100. * np.mean(test_losses), 2)))\n",
    "                print('Test accuracy: {:<5.2f}%\\n'.format(round(test_acc, 2)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
