{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py imports\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from corpora import MultiNLI, SciTail, StanfordNLI, AllNLI, BreakingNLI\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from embeddings import SentenceEmbedding\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from setuptools import setup\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "label_dict = {\"entailment\":0,\n",
    "             \"neutral\":1,\n",
    "             \"contradiction\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuda = False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "seed = 1\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"vector_cache/glove.840B.300d.txt\"\n",
    "word_vectors = preprocess_nli.load_glove(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict(zip([*word_vectors.keys()][:5], range(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens):\n",
    "    token2id = dict(zip([*word_vectors.keys()], range(2, 2+len(word_vectors))))\n",
    "    token2id[\"<PAD>\"] = 0\n",
    "    token2id[\"<UNK>\"] = 1\n",
    "    id2token = dict(zip(range(2, 2+len(word_vectors)),[*word_vectors.keys()]))\n",
    "    id2token[0] = \"<PAD>\"\n",
    "    id2token[1] = \"<UNK>\"\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_tensor = torch.from_numpy(np.array(pd.DataFrame(word_vectors).T)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_enli(nli_corpus = \"snli\"):\n",
    "\tif nli_corpus == \"snli\":\n",
    "\t\tpath_ = \"data/snli/snli_1.0/snli_1.0\"\n",
    "\telif nli_corpus == \"multinli\":\n",
    "\t\tpath_ = \"data/multinli/multinli_1.0/\"\n",
    "\n",
    "\ttrain = pd.read_json(\"{}_{}.jsonl\".format(path_,\"train\"), lines=True)\n",
    "\tdev = pd.read_json(\"{}_{}.jsonl\".format(path_,\"dev\"), lines=True)\n",
    "\ttest = pd.read_json(\"{}_{}.jsonl\".format(path_,\"test\"), lines=True)\n",
    "    \n",
    "\ttrain = train[train[\"gold_label\"] != \"-\"]\n",
    "\tdev = dev[dev[\"gold_label\"] != \"-\"]\n",
    "\ttest = test[test[\"gold_label\"] != \"-\"]\n",
    "\treturn train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_train, nli_dev, nli_test = read_enli(nli_corpus = \"snli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entailment       183416\n",
       "contradiction    183187\n",
       "neutral          182764\n",
       "Name: gold_label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train[\"gold_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_numeric_label(train, dev, test):\n",
    "    for dataset in [train, dev, test]:\n",
    "        dataset[\"gold_label\"] = dataset[\"gold_label\"].apply(lambda x: label_dict[x])\n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_train, nli_dev, nli_test = write_numeric_label(nli_train, nli_dev, nli_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "def tokenize_enli(dataset, remove_punc=False):\n",
    "    punc = string.punctuation\n",
    "    all_s1_tokens = []\n",
    "    all_s2_tokens = []\n",
    "    for s in [1,2]:\n",
    "        if remove_punc:\n",
    "            dataset[\"sentence{}_tokenized\".format(s)] = dataset[\"sentence{}\".format(s)].\\\n",
    "            apply(lambda x: reg.sub(\"\", x).lower().split(\" \"))\n",
    "        else:\n",
    "            dataset[\"sentence{}_tokenized\".format(s)] = dataset[\"sentence{}\".format(s)].\\\n",
    "            apply(lambda x: (reg.sub(\"\", x) + \" .\").lower().split(\" \"))\n",
    "    print (\"Tokenizing data.\")\n",
    "    dataset[\"sentence1_tokenized\"].apply(lambda x: all_s1_tokens.extend(x))\n",
    "    dataset[\"sentence2_tokenized\"].apply(lambda x: all_s2_tokens.extend(x))\n",
    "    all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    return dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data.\n",
      "Tokenizing data.\n",
      "Tokenizing data.\n"
     ]
    }
   ],
   "source": [
    "nli_train, all_train_tokens = tokenize_enli(nli_train, remove_punc=False)\n",
    "nli_dev, _ = tokenize_enli(nli_dev, remove_punc=False)\n",
    "nli_test, _ = tokenize_enli(nli_test, remove_punc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name):\n",
    "    try:\n",
    "        os.makedirs(name)\n",
    "    except OSError as ex:\n",
    "        if ex.errno == errno.EEXIST and os.path.isdir(name):\n",
    "            # ignore existing directory\n",
    "            pass\n",
    "        else:\n",
    "            # a different error happened\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset, max_sentence_length, token2id, id2token):\n",
    "        self.sentence1, self.sentence2, self.labels = [*tokenized_dataset[\"sentence1_tokenized\"].values], \\\n",
    "                                                      [*tokenized_dataset[\"sentence2_tokenized\"].values], \\\n",
    "                                                      [*tokenized_dataset[\"gold_label\"].values]\n",
    "        self.max_sentence_length = int(max_sentence_length)\n",
    "        self.token2id, self.id2token = token2id, id2token\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, row):\n",
    "        label = self.labels[row]\n",
    "        sentence1_word_idx, sentence2_word_idx = [], []\n",
    "        sentence1_mask, sentence2_mask = [], []\n",
    "        for word in self.sentence1[row][:self.max_sentence_length]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence1_word_idx.append(self.token2id[word])\n",
    "                sentence1_mask.append(0)\n",
    "            else:\n",
    "                sentence1_word_idx.append(UNK_IDX)\n",
    "                sentence1_mask.append(1)\n",
    "        for word in self.sentence2[row][:self.max_sentence_length]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence2_word_idx.append(self.token2id[word])\n",
    "                sentence2_mask.append(0)\n",
    "            else:\n",
    "                sentence2_word_idx.append(UNK_IDX)\n",
    "                sentence2_mask.append(1)\n",
    "        sentence1_list = [sentence1_word_idx, sentence1_mask, len(sentence1_word_idx)]\n",
    "        sentence2_list = [sentence2_word_idx, sentence2_mask, len(sentence2_word_idx)]\n",
    "        \n",
    "        return sentence1_list + sentence2_list + [label]\n",
    "\n",
    "def nli_collate_func(batch, max_sent_length):\n",
    "    sentence1_data, sentence2_data = [], []\n",
    "    sentence1_mask, sentence2_mask = [], []\n",
    "    s1_lengths, s2_lengths = [], []\n",
    "    labels = []\n",
    "\n",
    "    for datum in batch:\n",
    "        s1_lengths.append(datum[2])\n",
    "        s2_lengths.append(datum[5])\n",
    "        labels.append(datum[6])\n",
    "        sentence1_data_padded = np.pad(np.array(datum[0]), pad_width=((0, config.max_sentence_length-datum[2])), mode=\"constant\", constant_values=0)\n",
    "        sentence1_data.append(sentence1_data_padded)\n",
    "        sentence1_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, config.max_sentence_length-datum[2])), mode=\"constant\", constant_values=0)\n",
    "        sentence1_mask.append(sentence1_mask_padded)\n",
    "        sentence2_data_padded = np.pad(np.array(datum[3]), pad_width=((0, config.max_sentence_length-datum[5])), mode=\"constant\", constant_values=0)\n",
    "        sentence2_data.append(sentence2_data_padded)\n",
    "        sentence2_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, config.max_sentence_length-datum[5])), mode=\"constant\", constant_values=0)\n",
    "        sentence2_mask.append(sentence2_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(s1_lengths)[::-1]\n",
    "    sentence1_data = np.array(sentence1_data)[ind_dec_order]\n",
    "    sentence2_data = np.array(sentence2_data)[ind_dec_order]\n",
    "    sentence1_mask = np.array(sentence1_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    sentence2_mask = np.array(sentence2_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    s1_lengths = np.array(s1_lengths)[ind_dec_order]\n",
    "    s2_lengths = np.array(s2_lengths)[ind_dec_order]\n",
    "    labels = np.array(labels)[ind_dec_order]\n",
    "    \n",
    "    s1_list = [torch.from_numpy(sentence1_data), torch.from_numpy(sentence1_mask).float(), s1_lengths]\n",
    "    s2_list = [torch.from_numpy(sentence2_data), torch.from_numpy(sentence2_mask).float(), s2_lengths]\n",
    "        \n",
    "    return [torch.from_numpy(sentence1_data), torch.from_numpy(sentence1_mask).float(), s1_lengths,\n",
    "            torch.from_numpy(sentence2_data), torch.from_numpy(sentence2_mask).float(), s2_lengths,\n",
    "            labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config_class:\n",
    "    def __init__(self, max_sentence_length, corpus, epochs, batch_size, encoder_type, \n",
    "                 activation, optimizer,\n",
    "                 embed_dim, fc_dim, hidden_dim, layers, dropout, learning_rate,\n",
    "                 lr_patience, lr_decay, lr_reduction_factor, weight_decay,\n",
    "                 preserve_case, word_embedding, resume_snapshot, early_stopping_patience,\n",
    "                 save_path, seed):\n",
    "        \n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.corpus = corpus\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_type = encoder_type\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.embed_dim = embed_dim\n",
    "        self.fc_dim = fc_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_patience = lr_patience\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_reduction_factor = lr_reduction_factor\n",
    "        self.weight_decay = weight_decay\n",
    "        self.preserve_case = preserve_case\n",
    "        self.word_embedding = word_embedding\n",
    "        self.resume_snapshot = resume_snapshot\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.save_path = save_path\n",
    "        self.seed = seed\n",
    "        self.lower = True\n",
    "        self.vectors = word_vector_tensor\n",
    "        self.embed_size = self.vectors.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_class(30, \"snli\", 20, 16, \"HBMP\", \"relu\", \"adam\", 300, 300, 300, # default 600, 600 fc_dim, hidden_dim\n",
    "                      1, 0, 5e-4, 1, 0.99, 0.2, 0,\"store_false\", 'glove.840B.300d',\n",
    "                      \"\", 3, \"results\", 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "nli_train_dataset = NLIDataset(nli_train, max_sentence_length=config.max_sentence_length, token2id=token2id, id2token=id2token)\n",
    "nli_train_loader = torch.utils.data.DataLoader(dataset=nli_train_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sentence_length: nli_collate_func(x, config.max_sentence_length),\n",
    "                               shuffle=False)\n",
    "\n",
    "# dev\n",
    "nli_dev_dataset = NLIDataset(nli_dev, max_sentence_length=config.max_sentence_length, token2id=token2id, id2token=id2token)\n",
    "nli_dev_loader = torch.utils.data.DataLoader(dataset=nli_dev_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sentence_length: nli_collate_func(x, config.max_sentence_length),\n",
    "                               shuffle=False)\n",
    "\n",
    "# test\n",
    "nli_test_dataset = NLIDataset(nli_test, max_sentence_length=config.max_sentence_length, token2id=token2id, id2token=id2token)\n",
    "nli_test_loader = torch.utils.data.DataLoader(dataset=nli_test_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sentence_length: nli_collate_func(x, config.max_sentence_length),\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class biLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 embedding_weights,\n",
    "                 percent_dropout,\n",
    "                 vocab_size=word_vector_tensor.size(0),\n",
    "                 interaction_type=\"concat\",\n",
    "                 num_layers=1,\n",
    "                 input_size=300):\n",
    "\n",
    "        super(biLSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        embed_table = word_vector_tensor\n",
    "        embedding = nn.Embedding.from_pretrained(embed_table)\n",
    "        self.embedding = embedding\n",
    "        self.interaction = interaction_type\n",
    "        self.dropout = percent_dropout\n",
    "        self.LSTM = nn.LSTM(300, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        if self.LSTM.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        self.drop_out = nn.Dropout(self.dropout)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, c_0\n",
    "    \n",
    "    def forward(self, sentence, mask, lengths):\n",
    "        sort_original = sorted(range(len(lengths)), key=lambda sentence: -lengths[sentence])\n",
    "        unsort_to_original = sorted(range(len(lengths)), key=lambda sentence: sort_original[sentence])\n",
    "        sentence = sentence[sort_original]\n",
    "        _mask = mask[sort_original]\n",
    "        lengths = lengths[sort_original]\n",
    "        batch_size, seq_len = sentence.size()\n",
    "        self.hidden, self.c_0 = self.init_hidden(batch_size)\n",
    "        embeds = self.embedding(sentence)\n",
    "        embeds = mask*embeds + (1-_mask)*embeds.clone().detach()\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "        lstm_out, (self.hidden, self.c_0) = self.LSTM(embeds, (self.hidden, self.c_0))\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        lstm_out = lstm_out.view(batch_size, -1, 2, self.hidden_size)\n",
    "        lstm_out = torch.sum(lstm_out, dim=1)\n",
    "        lstm_out = torch.cat([lstm_out[:,i,:] for i in range(2)], dim=1)\n",
    "        lstm_out = lstm_out[unsort_to_original]\n",
    "        return lstm_out\n",
    "    \n",
    "    \n",
    "class Linear_Layers(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, hidden_size_2, percent_dropout,\n",
    "                 interaction_type=\"concat\", classes=3, input_size=300):\n",
    "        \n",
    "        super(Linear_Layers, self).__init__()\n",
    "        self.interaction = interaction_type\n",
    "        self.num_classes = classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.percent_dropout = percent_dropout\n",
    "        self.num_classes = classes\n",
    "        \n",
    "        if self.interaction == \"concat\":\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(4*self.hidden_size, self.hidden_size_2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(self.hidden_size_2, self.num_classes))\n",
    "        else:\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(2*self.hidden_size, self.hidden_size_2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(self.hidden_size_2, self.num_classes))\n",
    "\n",
    "        self.init_weights()\n",
    "        self.batch_norm = nn.BatchNorm1d(self.hidden_size * 4)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                nn.init.uniform_(module.bias)\n",
    "\n",
    "    def forward(self, lstm_out_1, lstm_out_2):\n",
    "        if self.interaction == \"concat\":\n",
    "            hidden = torch.cat([lstm_out_1, lstm_out_2], dim=1)\n",
    "        elif self.interaction == \"mul\":\n",
    "            hidden = lstm_out_1*lstm_out_2\n",
    "        elif self.interaction == \"subtract\":\n",
    "            hidden = lstm_out_1-lstm_out_2\n",
    "        hidden = self.batch_norm(hidden)\n",
    "        hidden = hidden.view(hidden.size(0),-1) \n",
    "        out = self.mlp(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(RNN, Linear_Classifier, DataLoader, criterion, optimizer, epoch):\n",
    "    \n",
    "    RNN.train()\n",
    "    Linear_Classifier.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (sentence1, s1_original, sentence1_lengths, \n",
    "                    sentence2, s2_original, sentence2_lengths, labels)\\\n",
    "    in enumerate(DataLoader):\n",
    "        sentence1, s1_original = sentence1.to(device), s1_original.to(device),  \n",
    "        sentence2, s2_original = sentence2.to(device), s2_original.to(device),\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        RNN.train()\n",
    "        Linear_Classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        output_s1 = RNN(sentence1, s1_original, sentence1_lengths)\n",
    "        output_s2 = RNN(sentence2, s2_original, sentence2_lengths)\n",
    "        out = Linear_Classifier(output_s1, output_s2)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.cuda().backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(sentence1) / len(DataLoader.dataset)\n",
    "        \n",
    "        if (batch_idx+1) % (len(DataLoader.dataset)//(20*labels.shape[0])) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1) * labels.shape[0], len(DataLoader.dataset),\n",
    "                100. * (batch_idx+1) / len(DataLoader), loss.item()), end=\"\\r\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def test(RNN, Linear_Classifier, DataLoader, criterion):\n",
    "\n",
    "    RNN.eval()\n",
    "    Linear_Classifier.eval()\n",
    "    test_loss = 0\n",
    "    label_list = []\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sentence1, s1_original, sentence1_lengths, \n",
    "                    sentence2, s2_original, sentence2_lengths, labels)\\\n",
    "                    in enumerate(DataLoader):\n",
    "\n",
    "            sentence1, s1_original = sentence1.to(device), s1_original.to(device),  \n",
    "            sentence2, s2_original = sentence2.to(device), s2_original.to(device),\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "            output_s1 = RNN(sentence1, s1_original, sentence1_lengths)\n",
    "            output_s2 = RNN(sentence2, s2_original, sentence2_lengths)\n",
    "            out = Linear_Classifier(output_s1, output_s2)\n",
    "            loss = criterion(out, labels)\n",
    "            test_loss += loss.item()/len(DataLoader.dataset)\n",
    "            output_list.append(out)\n",
    "            label_list.append(labels)\n",
    "            \n",
    "    return test_loss, torch.cat(output_list, dim=0), torch.cat(label_list, dim=0)\n",
    "\n",
    "def accuracy(RNN, Linear_Classifier, DataLoader, criterion):\n",
    "    \n",
    "    _, predicted, true_labels = test(RNN = RNN,  Linear_Classifier = Linear_Classifier,\n",
    "                                     DataLoader = DataLoader, criterion = criterion)\n",
    "\n",
    "    predicted = predicted.max(1)[1]\n",
    "    return 100 * predicted.eq(true_labels.data.view_as(predicted)).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "num_classes = 3\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "lstm_hidden_size = 512\n",
    "classifier_hidden_size = 512\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "lr = 5e-4\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding_weights(vectors, token2id, id2token, embedding_size):\n",
    "    weights = np.zeros((len(token2id), embedding_size))\n",
    "    for idx in range(2, len(id2token)):\n",
    "        token = id2token[idx]\n",
    "        weights[idx] = np.array(token2id[token])\n",
    "    weights[1] = np.random.randn(embedding_size)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN:\n",
      " biLSTM(\n",
      "  (embedding): Embedding(2196016, 300)\n",
      "  (LSTM): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      "  (drop_out): Dropout(p=0.1)\n",
      ")\n",
      "linear_model:\n",
      " Linear_Layers(\n",
      "  (mlp): Sequential(\n",
      "    (0): Dropout(p=0.1)\n",
      "    (1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.1)\n",
      "    (4): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      "  (batch_norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "epoch = 0\n",
      "Train Epoch: 0 [549120/549367 (100%)]\tLoss: 0.770704\n",
      "Validation Accuracy = 68.51249933242798\n",
      "epoch = 1\n",
      "Train Epoch: 1 [549120/549367 (100%)]\tLoss: 0.793327\n",
      "Validation Accuracy = 71.0221529006958\n",
      "epoch = 2\n",
      "Train Epoch: 2 [549120/549367 (100%)]\tLoss: 0.809516\n",
      "Validation Accuracy = 71.65210247039795\n",
      "epoch = 3\n",
      "Train Epoch: 3 [549120/549367 (100%)]\tLoss: 0.764643\n",
      "Validation Accuracy = 72.60719537734985\n",
      "epoch = 4\n",
      "Train Epoch: 4 [549120/549367 (100%)]\tLoss: 0.543374\n",
      "Validation Accuracy = 72.71896004676819\n",
      "epoch = 5\n",
      "Train Epoch: 5 [549120/549367 (100%)]\tLoss: 0.502929\n",
      "Validation Accuracy = 73.42003583908081\n",
      "epoch = 6\n",
      "Train Epoch: 6 [549120/549367 (100%)]\tLoss: 0.733698\n",
      "Validation Accuracy = 72.93233275413513\n",
      "epoch = 7\n",
      "Train Epoch: 7 [219648/549367 (40%)]\tLoss: 0.433071\r"
     ]
    }
   ],
   "source": [
    "weights_init = init_embedding_weights(word_vector_tensor, token2id, id2token, embedding_size = 300)\n",
    "\n",
    "RNN = biLSTM(hidden_size=lstm_hidden_size, num_layers=1, percent_dropout = 0.1, \n",
    "             embedding_weights = weights_init, vocab_size=word_vector_tensor.size(0),\n",
    "             interaction_type=\"concat\", input_size=300).to(device)\n",
    "\n",
    "linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 512,\n",
    "                             percent_dropout = 0.1, interaction_type=\"concat\", \n",
    "                             classes=3, input_size=300).to(device)\n",
    "\n",
    "print (\"RNN:\\n\", RNN)\n",
    "print (\"linear_model:\\n\", linear_model)\n",
    "\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss_train = train(RNN, linear_model, DataLoader = nli_train_loader,\n",
    "                       criterion = nn.CrossEntropyLoss(),\n",
    "                       optimizer = torch.optim.Adam(list(RNN.parameters()) + \\\n",
    "                                                   list(linear_model.parameters()), \n",
    "                                                   lr=5e-4), \n",
    "                      epoch = epoch)\n",
    "\n",
    "    loss_val, val_preds, val_true = test(RNN, linear_model, DataLoader = nli_dev_loader,\n",
    "                                         criterion = nn.CrossEntropyLoss(reduction='sum'))\n",
    "#     train_acc = accuracy(RNN, linear_model, nli_train_loader, nn.CrossEntropyLoss(reduction='sum'))\n",
    "    val_acc = accuracy(RNN, linear_model, nli_dev_loader, nn.CrossEntropyLoss(reduction='sum'))\n",
    "#     print (\"Train Accuracy = {}\".format(train_acc))\n",
    "    print (\"\\nValidation Accuracy = {}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adaptive hierarchical bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
