{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import csv\n",
    "import sys\n",
    "import errno\n",
    "import glob\n",
    "import string\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from setuptools import setup\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "label_dict = {\"entailment\":0, \"neutral\":1, \"contradiction\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuda = False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "seed = 1\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_path = \"/scratch/adc563/nlu_project/data/opus\"\n",
    "europarl_path = \"/scratch/adc563/nlu_project/data/europarl\"\n",
    "un_path = \"/scratch/adc563/nlu_project/data/un_parallel_corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xnli(lang):\n",
    "    fname = \"/scratch/adc563/nlu_project/data/XNLI/xnli.{}.jsonl\"\n",
    "    xnli_dev = pd.read_json(fname.format(\"dev\"), lines=True)\n",
    "    xnli_test = pd.read_json(fname.format(\"test\"), lines=True)\n",
    "    if lang == \"all\":\n",
    "        dev_data = xnli_dev\n",
    "        test_data = xnli_test\n",
    "    else:\n",
    "        dev_data = xnli_dev[xnli_dev[\"language\"]==lang]\n",
    "        test_data = xnli_test[xnli_test[\"language\"]==lang]\n",
    "    return dev_data, test_data\n",
    "\n",
    "def load_aligned_vectors(lang):\n",
    "    f = \"/scratch/adc563/nlu_project/data/aligned_embeddings/wiki.{}.align.vec\".format(lang)\n",
    "    fin = io.open(f, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(\" \")\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data\n",
    "\n",
    "def load_multilingual_vectors(lang):\n",
    "    f = \"/scratch/adc563/nlu_project/data/multi_lingual_embeddings/cc.{}.300.vec\".format(lang)\n",
    "    fin = io.open(f, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(\" \")\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data\n",
    "\n",
    "def load_glove_vectors(lang):\n",
    "    f = \"/scratch/adc563/nlu_project/HBMP/vector_cache/glove.840B.300d.txt\".format(lang)\n",
    "    fin = io.open(f, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
    "    n = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(\" \")\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data\n",
    "\n",
    "def read_enli(nli_corpus = \"snli\"):\n",
    "    if nli_corpus == \"snli\":\n",
    "        path_ = \"/scratch/adc563/nlu_project/HBMP/data/snli/snli_1.0/snli_1.0\"\n",
    "        train = pd.read_json(\"{}_{}.jsonl\".format(path_,\"train\"), lines=True)\n",
    "        dev = pd.read_json(\"{}_{}.jsonl\".format(path_,\"dev\"), lines=True)\n",
    "        test = pd.read_json(\"{}_{}.jsonl\".format(path_,\"test\"), lines=True)\n",
    "        # remove - from gold label\n",
    "        train = train[train[\"gold_label\"] != \"-\"]\n",
    "        dev = dev[dev[\"gold_label\"] != \"-\"]\n",
    "        test = test[test[\"gold_label\"] != \"-\"]\n",
    "    elif nli_corpus == \"multinli\":\n",
    "        path_ = \"/scratch/adc563/nlu_project/HBMP/data/multinli/multinli_1.0/multinli_1.0\"\n",
    "        train = pd.read_json(\"{}_{}.jsonl\".format(path_,\"train\"), lines=True)\n",
    "        dev = pd.read_json(\"{}_{}_matched.jsonl\".format(path_, \"dev\"), lines=True)\n",
    "        test = None\n",
    "        # remove - from gold label\n",
    "        train = train[train[\"gold_label\"] != \"-\"]\n",
    "        dev = dev[dev[\"gold_label\"] != \"-\"]\n",
    "    return train, dev, test\n",
    "\n",
    "def write_numeric_label(train, dev, test, nli_corpus=\"multinli\"):\n",
    "    if nli_corpus == \"multinli\":\n",
    "        for dataset in [train, dev]:\n",
    "            dataset[\"gold_label\"] = dataset[\"gold_label\"].apply(lambda x: label_dict[x])\n",
    "    elif nli_corpus == \"snli\":\n",
    "        for dataset in [train, dev, test]:\n",
    "            dataset[\"gold_label\"] = dataset[\"gold_label\"].apply(lambda x: label_dict[x])\n",
    "    elif nli_corpus == \"xnli\":\n",
    "        for dataset in [dev, test]:\n",
    "            dataset[\"gold_label\"] = dataset[\"gold_label\"].apply(lambda x: label_dict[x])\n",
    "    else:\n",
    "        raise ValueError (\"NLI corpus name should be in [multinli, snli, xnli]\")\n",
    "    return train, dev, test\n",
    "\n",
    "def tokenize_xnli(dataset, remove_punc=False, lang=\"en\"):\n",
    "    all_s1_tokens = []\n",
    "    all_s2_tokens = []\n",
    "    for s in [\"sentence1\", \"sentence2\"]:\n",
    "        punc = [*string.punctuation]\n",
    "        dataset[\"{}_tokenized\".format(s)] = dataset[\"{}\".format(s)].\\\n",
    "        apply(lambda x: \"\".join(c for c in x if c not in string.punctuation).lower().split(\" \"))\n",
    "        dataset[\"{}_tokenized\".format(s)] = dataset[\"{}_tokenized\".format(s)].\\\n",
    "        apply(lambda x: [a+\".\"+lang for a in x])\n",
    "    ext = dataset[\"sentence1_tokenized\"].apply(lambda x: all_s1_tokens.extend(x))\n",
    "    ext1 = dataset[\"sentence2_tokenized\"].apply(lambda x: all_s2_tokens.extend(x))\n",
    "    all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    return dataset, all_tokens\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = [*vocab]\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))\n",
    "    id2token = ['<PAD>', '<UNK>'] + id2token\n",
    "    token2id[\"<PAD>\"] = 0\n",
    "    token2id[\"<UNK>\"] = 1\n",
    "    return token2id, id2token\n",
    "\n",
    "\n",
    "\n",
    "def build_tok2id(id2token):\n",
    "    token2id = {}\n",
    "    for i in range(len(id2token)):\n",
    "        token2id[id2token[i]] = i\n",
    "    return token2id\n",
    "\n",
    "def init_embedding_weights(vectors, token2id, id2token, embedding_size):\n",
    "    weights = np.zeros((len(id2token), embedding_size))\n",
    "    for idx in range(2, len(id2token)):\n",
    "        token = id2token[idx]\n",
    "        weights[idx] = vectors[token]\n",
    "    weights[1] = np.random.randn(embedding_size)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIconfig:\n",
    "    def __init__(self, corpus, val_test_lang, max_sent_len, max_vocab_size, epochs, batch_size, \n",
    "                    embed_dim, hidden_dim, dropout, lr, experiment_lang):\n",
    "        self.corpus = corpus\n",
    "        self.val_test_lang = val_test_lang\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.experiment_lang = experiment_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NLIconfig(corpus = \"multinli\",\n",
    "             val_test_lang = \"en\",\n",
    "             max_sent_len = 40,\n",
    "             max_vocab_size = 200000,\n",
    "             epochs = 15,\n",
    "             batch_size = 32,\n",
    "             embed_dim = 300,\n",
    "             hidden_dim = 512,\n",
    "             dropout = 0.09,\n",
    "             lr = 1e-3,\n",
    "             experiment_lang = \"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocab_keys(src_vocab, trg_vocab):\n",
    "    for x in [*src_vocab.keys()]:\n",
    "        src_vocab[x + \".en\"] = src_vocab[x]\n",
    "        src_vocab.pop(x)\n",
    "    for y in [*trg_vocab.keys()]:\n",
    "        trg_vocab[y + \".{}\".format(config.experiment_lang)] = trg_vocab[y]\n",
    "        trg_vocab.pop(y)\n",
    "        \n",
    "    src_vocab.update(trg_vocab)\n",
    "    return src_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors for EN.\n",
      "Loading vectors for RU.\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading vectors for EN.\")\n",
    "aligned_src_vectors = load_glove_vectors(\"en\")\n",
    "print (\"Loading vectors for {}.\".format(config.experiment_lang.upper()))\n",
    "aligned_trg_vectors = load_aligned_vectors(config.experiment_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token_src = [x+\".\"+\"en\" for x in [*aligned_src_vectors.keys()]][:config.max_vocab_size]\n",
    "id2token_trg = [x+\".{}\".format(config.experiment_lang) for x in [*aligned_trg_vectors.keys()]][:config.max_vocab_size]\n",
    "id2token_mutual = [\"<PAD>\", \"<UNK>\"] + id2token_src + id2token_trg\n",
    "vecs_mutual = update_vocab_keys(aligned_src_vectors, aligned_trg_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_mutual = build_tok2id(id2token_mutual)\n",
    "weights_init = init_embedding_weights(vecs_mutual, token2id_mutual, id2token_mutual, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading multinli data.\n",
      "Writing numeric label.\n",
      "Reading XNLI EN data.\n"
     ]
    }
   ],
   "source": [
    "# load train and preprocess\n",
    "print (\"Reading {} data.\".format(config.corpus))\n",
    "nli_train, nli_dev, nli_test = read_enli(nli_corpus=config.corpus)\n",
    "print (\"Writing numeric label.\")\n",
    "if config.corpus == \"multinli\":\n",
    "    nli_train, nli_dev, _ = write_numeric_label(nli_train, nli_dev, nli_test, nli_corpus=config.corpus)\n",
    "elif config.corpus == \"snli\":\n",
    "    nli_train, nli_dev, nli_test = write_numeric_label(nli_train, nli_dev, nli_test, nli_corpus=config.corpus)\n",
    "\n",
    "# load val and test and preprocess\n",
    "print (\"Reading XNLI {} data.\".format(config.val_test_lang.upper()))\n",
    "xnli_dev, xnli_test = read_xnli(config.val_test_lang)\n",
    "_, xnli_dev, xnli_test = write_numeric_label(None, xnli_dev, xnli_test, nli_corpus=\"xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_train, all_train_tokens = tokenize_xnli(nli_train, lang=\"en\")\n",
    "# nli_dev = tokenize_enli(nli_dev)\n",
    "xnli_dev, _ = tokenize_xnli(xnli_dev, lang=\"en\")\n",
    "xnli_test, _ = tokenize_xnli(xnli_test, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "      <th>sentence1_tokenized</th>\n",
       "      <th>sentence2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>1</td>\n",
       "      <td>31193n</td>\n",
       "      <td>31193</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>( ( Conceptually ( cream skimming ) ) ( ( has ...</td>\n",
       "      <td>(ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>( ( ( Product and ) geography ) ( ( are ( what...</td>\n",
       "      <td>(ROOT (S (NP (NN Product) (CC and) (NN geograp...</td>\n",
       "      <td>[conceptually.en, cream.en, skimming.en, has.e...</td>\n",
       "      <td>[product.en, and.en, geography.en, are.en, wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>0</td>\n",
       "      <td>101457e</td>\n",
       "      <td>101457</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>( you ( ( know ( during ( ( ( the season ) and...</td>\n",
       "      <td>(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>( You ( ( ( ( lose ( the things ) ) ( to ( the...</td>\n",
       "      <td>(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...</td>\n",
       "      <td>[you.en, know.en, during.en, the.en, season.en...</td>\n",
       "      <td>[you.en, lose.en, the.en, things.en, to.en, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0</td>\n",
       "      <td>134793e</td>\n",
       "      <td>134793</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>( ( One ( of ( our number ) ) ) ( ( will ( ( (...</td>\n",
       "      <td>(ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>( ( ( A member ) ( of ( my team ) ) ) ( ( will...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...</td>\n",
       "      <td>[one.en, of.en, our.en, number.en, will.en, ca...</td>\n",
       "      <td>[a.en, member.en, of.en, my.en, team.en, will....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_labels       genre  gold_label   pairID  promptID  \\\n",
       "0        [neutral]  government           1   31193n     31193   \n",
       "1     [entailment]   telephone           0  101457e    101457   \n",
       "2     [entailment]     fiction           0  134793e    134793   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  Conceptually cream skimming has two basic dime...   \n",
       "1  you know during the season and i guess at at y...   \n",
       "2  One of our number will carry out your instruct...   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( Conceptually ( cream skimming ) ) ( ( has ...   \n",
       "1  ( you ( ( know ( during ( ( ( the season ) and...   \n",
       "2  ( ( One ( of ( our number ) ) ) ( ( will ( ( (...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...   \n",
       "1  (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...   \n",
       "2  (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( ( Product and ) geography ) ( ( are ( what...   \n",
       "1  ( You ( ( ( ( lose ( the things ) ) ( to ( the...   \n",
       "2  ( ( ( A member ) ( of ( my team ) ) ) ( ( will...   \n",
       "\n",
       "                                     sentence2_parse  \\\n",
       "0  (ROOT (S (NP (NN Product) (CC and) (NN geograp...   \n",
       "1  (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...   \n",
       "2  (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...   \n",
       "\n",
       "                                 sentence1_tokenized  \\\n",
       "0  [conceptually.en, cream.en, skimming.en, has.e...   \n",
       "1  [you.en, know.en, during.en, the.en, season.en...   \n",
       "2  [one.en, of.en, our.en, number.en, will.en, ca...   \n",
       "\n",
       "                                 sentence2_tokenized  \n",
       "0  [product.en, and.en, geography.en, are.en, wha...  \n",
       "1  [you.en, lose.en, the.en, things.en, to.en, th...  \n",
       "2  [a.en, member.en, of.en, my.en, team.en, will....  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset, max_sentence_length, token2id, id2token):\n",
    "        self.sentence1, self.sentence2, self.labels = [*tokenized_dataset[\"sentence1_tokenized\"].values], \\\n",
    "                                                      [*tokenized_dataset[\"sentence2_tokenized\"].values], \\\n",
    "                                                      [*tokenized_dataset[\"gold_label\"].values]\n",
    "        self.max_sentence_length = int(max_sentence_length)\n",
    "        self.token2id, self.id2token = token2id, id2token\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, row):\n",
    "        label = self.labels[row]\n",
    "        sentence1_word_idx, sentence2_word_idx = [], []\n",
    "        sentence1_mask, sentence2_mask = [], []\n",
    "        for word in self.sentence1[row][:self.max_sentence_length]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence1_word_idx.append(self.token2id[word])\n",
    "                sentence1_mask.append(0)\n",
    "            else:\n",
    "                sentence1_word_idx.append(UNK_IDX)\n",
    "                sentence1_mask.append(1)\n",
    "        for word in self.sentence2[row][:self.max_sentence_length]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence2_word_idx.append(self.token2id[word])\n",
    "                sentence2_mask.append(0)\n",
    "            else:\n",
    "                sentence2_word_idx.append(UNK_IDX)\n",
    "                sentence2_mask.append(1)\n",
    "        sentence1_list = [sentence1_word_idx, sentence1_mask, len(sentence1_word_idx)]\n",
    "        sentence2_list = [sentence2_word_idx, sentence2_mask, len(sentence2_word_idx)]\n",
    "        \n",
    "        return sentence1_list + sentence2_list + [label]\n",
    "\n",
    "def nli_collate_func(batch, max_sent_length):\n",
    "    sentence1_data, sentence2_data = [], []\n",
    "    sentence1_mask, sentence2_mask = [], []\n",
    "    s1_lengths, s2_lengths = [], []\n",
    "    labels = []\n",
    "\n",
    "    for datum in batch:\n",
    "        s1_lengths.append(datum[2])\n",
    "        s2_lengths.append(datum[5])\n",
    "        labels.append(datum[6])\n",
    "        sentence1_data_padded = np.pad(np.array(datum[0]), pad_width=((0, config.max_sent_len-datum[2])), mode=\"constant\", constant_values=0)\n",
    "        sentence1_data.append(sentence1_data_padded)\n",
    "        sentence1_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, config.max_sent_len-datum[2])), mode=\"constant\", constant_values=0)\n",
    "        sentence1_mask.append(sentence1_mask_padded)\n",
    "        sentence2_data_padded = np.pad(np.array(datum[3]), pad_width=((0, config.max_sent_len-datum[5])), mode=\"constant\", constant_values=0)\n",
    "        sentence2_data.append(sentence2_data_padded)\n",
    "        sentence2_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, config.max_sent_len-datum[5])), mode=\"constant\", constant_values=0)\n",
    "        sentence2_mask.append(sentence2_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(s1_lengths)[::-1]\n",
    "    sentence1_data = np.array(sentence1_data)[ind_dec_order]\n",
    "    sentence2_data = np.array(sentence2_data)[ind_dec_order]\n",
    "    sentence1_mask = np.array(sentence1_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    sentence2_mask = np.array(sentence2_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    s1_lengths = np.array(s1_lengths)[ind_dec_order]\n",
    "    s2_lengths = np.array(s2_lengths)[ind_dec_order]\n",
    "    labels = np.array(labels)[ind_dec_order]\n",
    "    \n",
    "    s1_list = [torch.from_numpy(sentence1_data), torch.from_numpy(sentence1_mask).float(), s1_lengths]\n",
    "    s2_list = [torch.from_numpy(sentence2_data), torch.from_numpy(sentence2_mask).float(), s2_lengths]\n",
    "        \n",
    "    return [torch.from_numpy(sentence1_data), torch.from_numpy(sentence1_mask).float(), s1_lengths,\n",
    "            torch.from_numpy(sentence2_data), torch.from_numpy(sentence2_mask).float(), s2_lengths,\n",
    "            labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "nli_train_dataset = NLIDataset(nli_train, max_sentence_length=config.max_sent_len, token2id=token2id_mutual, id2token=id2token_mutual)\n",
    "nli_train_loader = torch.utils.data.DataLoader(dataset=nli_train_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)\n",
    "\n",
    "# dev\n",
    "nli_dev_dataset = NLIDataset(xnli_dev, max_sentence_length=config.max_sent_len, token2id=token2id_mutual, id2token=id2token_mutual)\n",
    "nli_dev_loader = torch.utils.data.DataLoader(dataset=nli_dev_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)\n",
    "\n",
    "# test\n",
    "nli_test_dataset = NLIDataset(xnli_test, max_sentence_length=config.max_sent_len, token2id=token2id_mutual, id2token=id2token_mutual)\n",
    "nli_test_loader = torch.utils.data.DataLoader(dataset=nli_test_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class biLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 embedding_weights,\n",
    "                 percent_dropout,\n",
    "                 vocab_size,\n",
    "                 interaction_type=\"concat\",\n",
    "                 num_layers=1,\n",
    "                 input_size=300,\n",
    "                 src_trg = \"src\"):\n",
    "\n",
    "        super(biLSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        \n",
    "        self.embed_table = torch.from_numpy(embedding_weights).float()\n",
    "        embedding = nn.Embedding.from_pretrained(self.embed_table)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.interaction = interaction_type\n",
    "        self.dropout = percent_dropout\n",
    "        self.drop_out = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(300, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.LSTM2 = nn.LSTM(300, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.LSTM3 = nn.LSTM(300, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        if self.LSTM.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.bn = nn.BatchNorm1d(self.hidden_size * self.num_directions)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, c_0\n",
    "    \n",
    "    def forward(self, sentence, mask, lengths):\n",
    "        sort_original = sorted(range(len(lengths)), key=lambda sentence: -lengths[sentence])\n",
    "        unsort_to_original = sorted(range(len(lengths)), key=lambda sentence: sort_original[sentence])\n",
    "        \n",
    "        sentence = sentence[sort_original]\n",
    "        _mask = mask[sort_original]\n",
    "        lengths = lengths[sort_original]\n",
    "        batch_size, seq_len = sentence.size()\n",
    "        self.hidden, self.c_0 = self.init_hidden(batch_size)\n",
    "        \n",
    "        # embdddings\n",
    "        embeds = self.embedding(sentence)\n",
    "        embeds = mask*embeds + (1-_mask)*embeds.clone().detach()\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "        # first lstm\n",
    "        lstm_out, (self.hidden_1, self.c_1) = self.LSTM(embeds, (self.hidden, self.c_0))\n",
    "        emb1, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        emb1 = emb1.view(batch_size, -1, 2, self.hidden_size)\n",
    "        emb1 = torch.max(emb1, dim=1)[0]\n",
    "        emb1 = torch.cat([emb1[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "        emb1 = emb1[unsort_to_original]\n",
    "        \n",
    "        out = self.bn(emb1)\n",
    "        \n",
    "#         lstm_out_2, (self.hidden_2, self.c_2) = self.LSTM2(embeds, (self.hidden_1, self.c_1))\n",
    "#         lstm_out_2, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out_2, batch_first=True)\n",
    "#         lstm_out_2 = lstm_out_2.view(batch_size, -1, 2, self.hidden_size)\n",
    "        \n",
    "#         lstm_out_2 = torch.max(lstm_out_2, dim=1)[0]\n",
    "#         lstm_out_2 = torch.cat([lstm_out_2[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "#         lstm_out_2 = lstm_out_2[unsort_to_original]\n",
    "        \n",
    "#         lstm_out_3, (self.hidden_3, self.c_3) = self.LSTM3(embeds, (self.hidden_2, self.c_2))\n",
    "#         lstm_out_3, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out_3, batch_first=True)\n",
    "#         lstm_out_3 = lstm_out_3.view(batch_size, -1, 2, self.hidden_size)\n",
    "#         lstm_out_3 = torch.max(lstm_out_3, dim=1)[0]\n",
    "        \n",
    "#         lstm_out_3 = torch.cat([lstm_out_3[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "#         lstm_out_3 = lstm_out_3[unsort_to_original]\n",
    "#         out = torch.cat([emb1, lstm_out_2, lstm_out_3], dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Layers(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, hidden_size_2, percent_dropout,\n",
    "                 interaction_type=\"concat\", classes=3, input_size=300):\n",
    "        \n",
    "        super(Linear_Layers, self).__init__()\n",
    "        self.interaction = interaction_type\n",
    "        self.num_classes = classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.percent_dropout = percent_dropout\n",
    "        self.num_classes = classes\n",
    "        \n",
    "        if self.interaction == \"concat\":\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(4 * self.hidden_size, self.hidden_size_2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(self.hidden_size_2, self.num_classes))\n",
    "        else:\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(6 * self.hidden_size, self.hidden_size_2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(self.hidden_size_2, int(self.hidden_size_2/2)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(int(self.hidden_size_2/2), self.num_classes))\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                nn.init.uniform_(module.bias)\n",
    "\n",
    "    def forward(self, lstm_out_1, lstm_out_2):\n",
    "        if self.interaction == \"concat\":\n",
    "            hidden = torch.cat([lstm_out_1, lstm_out_2, torch.abs(lstm_out_1 - lstm_out_2), \n",
    "                                torch.mul(lstm_out_1, lstm_out_2)], dim=1)\n",
    "        elif self.interaction == \"mul\":\n",
    "            hidden = lstm_out_1*lstm_out_2\n",
    "        elif self.interaction == \"subtract\":\n",
    "            hidden = lstm_out_1-lstm_out_2\n",
    "        hidden = hidden.view(hidden.size(0),-1) \n",
    "        out = self.mlp(hidden)\n",
    "        out = F.log_softmax(out, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(RNN, Linear_Classifier, DataLoader, criterion, optimizer, epoch):\n",
    "    \n",
    "    RNN.train()\n",
    "    Linear_Classifier.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (sentence1, s1_original, sentence1_lengths, \n",
    "                    sentence2, s2_original, sentence2_lengths, labels)\\\n",
    "    in enumerate(DataLoader):\n",
    "        sentence1, s1_original = sentence1.to(device), s1_original.to(device),  \n",
    "        sentence2, s2_original = sentence2.to(device), s2_original.to(device),\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        RNN.train()\n",
    "        Linear_Classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        output_s1 = RNN(sentence1, s1_original, sentence1_lengths)\n",
    "        output_s2 = RNN(sentence2, s2_original, sentence2_lengths)\n",
    "        out = Linear_Classifier(output_s1, output_s2)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(sentence1) / len(DataLoader.dataset)\n",
    "        if (batch_idx+1) % (len(DataLoader.dataset)//(20*labels.shape[0])) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1) * labels.shape[0], len(DataLoader.dataset),\n",
    "                100. * (batch_idx+1) / len(DataLoader), loss.item()), end=\"\\r\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def test(RNN, Linear_Classifier, DataLoader, criterion):\n",
    "\n",
    "    RNN.eval()\n",
    "    Linear_Classifier.eval()\n",
    "    test_loss = 0\n",
    "    label_list = []\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sentence1, s1_original, sentence1_lengths, \n",
    "                    sentence2, s2_original, sentence2_lengths, labels)\\\n",
    "                    in enumerate(DataLoader):\n",
    "            sentence1, s1_original = sentence1.to(device), s1_original.to(device),  \n",
    "            sentence2, s2_original = sentence2.to(device), s2_original.to(device),\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "            output_s1 = RNN(sentence1, s1_original, sentence1_lengths)\n",
    "            output_s2 = RNN(sentence2, s2_original, sentence2_lengths)\n",
    "            out = Linear_Classifier(output_s1, output_s2)\n",
    "            loss = criterion(out, labels)\n",
    "            test_loss += loss.item()/len(DataLoader.dataset)\n",
    "            output_list.append(out)\n",
    "            label_list.append(labels)\n",
    "            \n",
    "    return test_loss, torch.cat(output_list, dim=0), torch.cat(label_list, dim=0)\n",
    "\n",
    "def accuracy(RNN, Linear_Classifier, DataLoader, criterion):\n",
    "    \n",
    "    _, predicted, true_labels = test(RNN = RNN,  Linear_Classifier = Linear_Classifier,\n",
    "                                     DataLoader = DataLoader, criterion = criterion)\n",
    "\n",
    "    predicted = predicted.max(1)[1]\n",
    "    return 100 * predicted.eq(true_labels.data.view_as(predicted)).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      " biLSTM(\n",
      "  (embedding): Embedding(400002, 300)\n",
      "  (drop_out): Dropout(p=0.09)\n",
      "  (LSTM): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      "  (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Classifier:\n",
      " Linear_Layers(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.1)\n",
      "    (3): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "epoch = 0\n",
      "Train Epoch: 0 [392320/392702 (100%)]\tLoss: 1.009115\n",
      "EN Validation Accuracy = 64.49799537658691\n",
      "\n",
      "epoch = 1\n",
      "Train Epoch: 1 [392320/392702 (100%)]\tLoss: 0.906441\n",
      "EN Validation Accuracy = 65.66265225410461\n",
      "\n",
      "epoch = 2\n",
      "Train Epoch: 2 [392320/392702 (100%)]\tLoss: 0.895451\n",
      "EN Validation Accuracy = 66.38554334640503\n",
      "\n",
      "epoch = 3\n",
      "Train Epoch: 3 [392320/392702 (100%)]\tLoss: 0.795442\n",
      "EN Validation Accuracy = 68.07228922843933\n",
      "\n",
      "epoch = 4\n",
      "Train Epoch: 4 [392320/392702 (100%)]\tLoss: 0.969998\n",
      "EN Validation Accuracy = 67.2690749168396\n"
     ]
    }
   ],
   "source": [
    "LSTM_en = biLSTM(config.hidden_dim, weights_init, config.dropout, config.max_vocab_size, \n",
    "                 interaction_type=\"concat\",\n",
    "                 num_layers=1, input_size=300, src_trg = \"src\").to(device)\n",
    "\n",
    "LSTM_en.embedding.requires_grad = False\n",
    "\n",
    "linear_model = Linear_Layers(hidden_size = 1024, hidden_size_2 = 128,\n",
    "                             percent_dropout = 0.1, interaction_type=\"concat\", \n",
    "                             classes=3, input_size=300).to(device)\n",
    "\n",
    "print (\"Encoder:\\n\", LSTM_en)\n",
    "print (\"Classifier:\\n\", linear_model)\n",
    "validation_accuracy = [0]\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, start_epoch + n_epochs):\n",
    "    print (\"\\nepoch = \"+str(epoch))\n",
    "    loss_train = train(LSTM_en, linear_model, DataLoader = nli_train_loader,\n",
    "                       criterion = nn.NLLLoss(),\n",
    "                       optimizer = torch.optim.Adam(list(LSTM_en.parameters()) + list(linear_model.parameters()), \n",
    "                                                   lr=config.lr, weight_decay=0),\n",
    "                       epoch = epoch)\n",
    "    val_acc = accuracy(LSTM_en, linear_model, nli_dev_loader, nn.NLLLoss(reduction='sum'))\n",
    "    print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), val_acc))\n",
    "    if val_acc <= validation_accuracy[-1]:\n",
    "        break\n",
    "    validation_accuracy.append(val_acc)\n",
    "    torch.save(LSTM_en.state_dict(), \"best_encoder_eng_mnli_{}_{}\".format(epoch, config.experiment_lang))\n",
    "    torch.save(linear_model.state_dict(), \"best_linear_eng_mnli_{}_{}\".format(epoch, config.experiment_lang))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
