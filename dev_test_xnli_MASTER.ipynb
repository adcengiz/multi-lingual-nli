{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import csv\n",
    "import sys\n",
    "import errno\n",
    "import glob\n",
    "import string\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import jieba\n",
    "import time\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from setuptools import setup\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "label_dict = {\"entailment\":0, \"neutral\":1, \"contradiction\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuda = False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "seed = 1\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xnli(lang):\n",
    "    fname = \"/scratch/adc563/nlu_project/data/XNLI/xnli.{}.jsonl\"\n",
    "    xnli_dev = pd.read_json(fname.format(\"dev\"), lines=True)\n",
    "    xnli_test = pd.read_json(fname.format(\"test\"), lines=True)\n",
    "    if lang == \"all\":\n",
    "        dev_data = xnli_dev\n",
    "        test_data = xnli_test\n",
    "    else:\n",
    "        dev_data = xnli_dev[xnli_dev[\"language\"]==lang]\n",
    "        test_data = xnli_test[xnli_test[\"language\"]==lang]\n",
    "    return dev_data, test_data\n",
    "\n",
    "def load_aligned_vectors(lang):\n",
    "    f = \"/scratch/adc563/nlu_project/data/aligned_embeddings/wiki.{}.align.vec\".format(lang)\n",
    "    fin = io.open(f, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(\" \")\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data\n",
    "\n",
    "def load_multilingual_vectors(lang):\n",
    "    fname = \"/scratch/adc563/nlu_project/data/multi_lingual_embeddings/cc.{}.300.vec\".format(lang)\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "def load_glove_vectors(lang):\n",
    "    f = \"/scratch/adc563/nlu_project/HBMP/vector_cache/glove.840B.300d.txt\".format(lang)\n",
    "    fin = io.open(f, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
    "    n = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(\" \")\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data\n",
    "\n",
    "def read_enli(nli_corpus = \"snli\"):\n",
    "    if nli_corpus == \"snli\":\n",
    "        path_ = \"/scratch/adc563/nlu_project/HBMP/data/snli/snli_1.0/snli_1.0\"\n",
    "        train = pd.read_json(\"{}_{}.jsonl\".format(path_,\"train\"), lines=True)\n",
    "        dev = pd.read_json(\"{}_{}.jsonl\".format(path_,\"dev\"), lines=True)\n",
    "        test = pd.read_json(\"{}_{}.jsonl\".format(path_,\"test\"), lines=True)\n",
    "        # remove - from gold label\n",
    "        train = train[train[\"gold_label\"] != \"-\"]\n",
    "        dev = dev[dev[\"gold_label\"] != \"-\"]\n",
    "        test = test[test[\"gold_label\"] != \"-\"]\n",
    "    elif nli_corpus == \"multinli\":\n",
    "        path_ = \"/scratch/adc563/nlu_project/HBMP/data/multinli/multinli_1.0/multinli_1.0\"\n",
    "        train = pd.read_json(\"{}_{}.jsonl\".format(path_,\"train\"), lines=True)\n",
    "        dev = pd.read_json(\"{}_{}_matched.jsonl\".format(path_, \"dev\"), lines=True)\n",
    "        test = None\n",
    "        # remove - from gold label\n",
    "        train = train[train[\"gold_label\"] != \"-\"]\n",
    "        dev = dev[dev[\"gold_label\"] != \"-\"]\n",
    "    return train, dev, test\n",
    "\n",
    "def write_numeric_label(train, dev, test, nli_corpus=\"multinli\"):\n",
    "    if nli_corpus == \"multinli\":\n",
    "        for dataset in [train, dev]:\n",
    "            dataset[\"gold_label\"] = dataset[\"gold_label\"].apply(lambda x: label_dict[x])\n",
    "    elif nli_corpus == \"snli\":\n",
    "        for dataset in [train, dev, test]:\n",
    "            dataset[\"gold_label\"] = dataset[\"gold_label\"].apply(lambda x: label_dict[x])\n",
    "    elif nli_corpus == \"xnli\":\n",
    "        for dataset in [dev, test]:\n",
    "            dataset[\"gold_label\"] = dataset[\"gold_label\"].apply(lambda x: label_dict[x])\n",
    "    else:\n",
    "        raise ValueError (\"NLI corpus name should be in [multinli, snli, xnli]\")\n",
    "    return train, dev, test\n",
    "\n",
    "def tokenize_xnli(dataset, remove_punc=False, lang=\"en\"):\n",
    "    all_s1_tokens = []\n",
    "    all_s2_tokens = []\n",
    "    punc = [*string.punctuation]\n",
    "    if lang == \"ar\":\n",
    "        for s in [\"sentence1\", \"sentence2\"]:\n",
    "            dataset[\"{}_tokenized\".format(s)] = dataset[s].\\\n",
    "            apply(lambda x: [a + \".ar\" for a in nltk.tokenize.wordpunct_tokenize(x)])\n",
    "        ext = dataset[\"sentence1_tokenized\"].apply(lambda x: all_s1_tokens.extend(x))\n",
    "        ext1 = dataset[\"sentence2_tokenized\"].apply(lambda x: all_s2_tokens.extend(x))\n",
    "        all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    elif lang == \"zh\":\n",
    "        for s in [\"sentence1\", \"sentence2\"]:\n",
    "            dataset[\"{}_tokenized\".format(s)] = dataset[s].\\\n",
    "            apply(lambda x: [z + \".zh\" for z in ' '.join(jieba.cut(x, cut_all=True)).split(\" \") if z not in string.punctuation])\n",
    "        ext = dataset[\"sentence1_tokenized\"].apply(lambda x: all_s1_tokens.extend(x))\n",
    "        ext1 = dataset[\"sentence2_tokenized\"].apply(lambda x: all_s2_tokens.extend(x))\n",
    "        all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    else:\n",
    "        for s in [\"sentence1\", \"sentence2\"]:\n",
    "            dataset[\"{}_tokenized\".format(s)] = dataset[s].\\\n",
    "            apply(lambda x: \"\".join(c for c in x if c not in string.punctuation).lower().split(\" \"))\n",
    "            dataset[\"{}_tokenized\".format(s)] = dataset[\"{}_tokenized\".format(s)].\\\n",
    "            apply(lambda x: [a+\".\"+lang for a in x])\n",
    "        ext = dataset[\"sentence1_tokenized\"].apply(lambda x: all_s1_tokens.extend(x))\n",
    "        ext1 = dataset[\"sentence2_tokenized\"].apply(lambda x: all_s2_tokens.extend(x))\n",
    "        all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    return dataset, all_tokens\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = [*vocab]\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))\n",
    "    id2token = ['<PAD>', '<UNK>'] + id2token\n",
    "    token2id[\"<PAD>\"] = 0\n",
    "    token2id[\"<UNK>\"] = 1\n",
    "    return token2id, id2token\n",
    "\n",
    "def build_tok2id(id2token):\n",
    "    token2id = {}\n",
    "    for i in range(len(id2token)):\n",
    "        token2id[id2token[i]] = i\n",
    "    return token2id\n",
    "\n",
    "def init_embedding_weights(vectors, token2id, id2token, embedding_size):\n",
    "    weights = np.zeros((len(id2token), embedding_size))\n",
    "    for idx in range(2, len(id2token)):\n",
    "        token = id2token[idx]\n",
    "        weights[idx] = vectors[token]\n",
    "    weights[1] = np.random.randn(embedding_size)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config_class:\n",
    "    def __init__(self, corpus, val_test_lang, max_sent_len, max_vocab_size, epochs, batch_size, \n",
    "                    embed_dim, hidden_dim, dropout, lr):\n",
    "        self.corpus = corpus\n",
    "        self.val_test_lang = val_test_lang\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_class(corpus = \"multinli\",\n",
    "             val_test_lang = \"zh\",\n",
    "             max_sent_len = 30,\n",
    "             max_vocab_size = 210000,\n",
    "             epochs = 15,\n",
    "             batch_size = 64, # decreased, because we will have contrastive batch - so x 2\n",
    "             embed_dim = 300,\n",
    "             hidden_dim = 512,\n",
    "             dropout = 0.1,\n",
    "             lr = 8e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocab_keys(src_vocab, trg_vocab):\n",
    "    for x in [*src_vocab.keys()]:\n",
    "        src_vocab[x + \".en\"] = src_vocab[x]\n",
    "        src_vocab.pop(x)\n",
    "    for y in [*trg_vocab.keys()]:\n",
    "        trg_vocab[y + \".{}\".format(config.val_test_lang)] = trg_vocab[y]\n",
    "        trg_vocab.pop(y)\n",
    "        \n",
    "    src_vocab.update(trg_vocab)\n",
    "    return src_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_path = \"/scratch/adc563/nlu_project/data/opus\"\n",
    "europarl_path = \"/scratch/adc563/nlu_project/data/europarl\"\n",
    "un_path = \"/scratch/adc563/nlu_project/data/un_parallel_corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors for EN.\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading vectors for EN.\")\n",
    "aligned_src_vectors = load_glove_vectors(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors for ZH.\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading vectors for {}.\".format(config.val_test_lang.upper()))\n",
    "aligned_trg_vectors = load_aligned_vectors(config.val_test_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token_src = [x+\".\"+\"en\" for x in [*aligned_src_vectors.keys()]][:config.max_vocab_size]\n",
    "id2token_trg = [x+\".\"+config.val_test_lang for x in [*aligned_trg_vectors.keys()]][:config.max_vocab_size]\n",
    "id2token_mutual = [\"<PAD>\", \"<UNK>\"] + id2token_src + id2token_trg\n",
    "vecs_mutual = update_vocab_keys(aligned_src_vectors, aligned_trg_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_mutual = build_tok2id(id2token_mutual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init = init_embedding_weights(vecs_mutual, token2id_mutual, id2token_mutual, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420002, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize_opus_data(lang=\"tr\"):\n",
    "    all_en_tokens = []\n",
    "    all_target_tokens = []\n",
    "    path_en = opus_path + \"/{}_en/en_data_00\".format(lang, lang)\n",
    "    path_target = opus_path + \"/{}_en/{}_data_00\".format(lang, lang)\n",
    "    en_corpus = open(path_en, \"r\")\n",
    "    target_corpus = open(path_target, \"r\")\n",
    "    en_series = pd.Series(en_corpus.read().split(\"\\n\"))\n",
    "    target_series = pd.Series(target_corpus.read().split(\"\\n\"))\n",
    "    dataset = pd.DataFrame({\"en\":en_series, lang:target_series})\n",
    "    if lang == \"ar\":\n",
    "        dataset[\"en_tokenized\"] = dataset[\"en\"].apply(lambda x: \"\".join(c for c in x if c not in string.punctuation).lower().split(\" \"))\n",
    "        dataset[\"en_tokenized\"] = dataset[\"en_tokenized\"].apply(lambda x:[a+\".en\" for a in x])\n",
    "        dataset[\"ar_tokenized\"] = dataset[\"ar\"].apply(lambda x: [a + \".ar\" for a in nltk.tokenize.wordpunct_tokenize(x)])\n",
    "    else:\n",
    "        for i in [\"en\", lang]:\n",
    "            dataset[\"{}_tokenized\".format(i)] = dataset[i].apply(lambda x: \"\".join(c for c in x if c not in string.punctuation).lower().split(\" \"))\n",
    "            dataset[\"{}_tokenized\".format(i)] = dataset[\"{}_tokenized\".format(i)].\\\n",
    "            apply(lambda x:[a+\".{}\".format(i) for a in x])\n",
    "    dataset[\"en_tokenized\"].apply(lambda x: all_en_tokens.extend(x))\n",
    "    dataset[\"{}_tokenized\".format(lang)].apply(lambda x: all_target_tokens.extend(x))\n",
    "    return dataset, all_en_tokens, all_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize_europarl_data(lang=\"de\"):\n",
    "    all_en_tokens = []\n",
    "    all_target_tokens = []\n",
    "    path_en = europarl_path + \"/{}_en/europarl-v7.{}-en.en\".format(lang, lang)\n",
    "    path_target = europarl_path + \"/{}_en/europarl-v7.{}-en.{}\".format(lang, lang, lang)\n",
    "    en_corpus = open(path_en, \"r\")\n",
    "    target_corpus = open(path_target, \"r\")\n",
    "    en_series = pd.Series(en_corpus.read().split(\"\\n\"))\n",
    "    target_series = pd.Series(target_corpus.read().split(\"\\n\"))\n",
    "    dataset = pd.DataFrame({\"en\":en_series, lang:target_series})\n",
    "    for i in [\"en\", lang]:\n",
    "        dataset[\"{}_tokenized\".format(i)] = dataset[i].apply(lambda x: \"\".join(c for c in x if c not in string.punctuation).lower().split(\" \"))\n",
    "        dataset[\"{}_tokenized\".format(i)] = dataset[\"{}_tokenized\".format(i)].apply(lambda x:[a+\".{}\".format(i) for a in x])\n",
    "    dataset[\"en_tokenized\"].apply(lambda x: all_en_tokens.extend(x))\n",
    "    dataset[\"{}_tokenized\".format(lang)].apply(lambda x: all_target_tokens.extend(x))\n",
    "    return dataset, all_en_tokens, all_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class biLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 embedding_weights,\n",
    "                 percent_dropout,\n",
    "                 vocab_size,\n",
    "                 interaction_type=\"concat\",\n",
    "                 num_layers=1,\n",
    "                 input_size=300,\n",
    "                 src_trg = \"src\"):\n",
    "\n",
    "        super(biLSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        \n",
    "        self.embed_table = torch.from_numpy(embedding_weights).float()\n",
    "        embedding = nn.Embedding.from_pretrained(self.embed_table)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.interaction = interaction_type\n",
    "        self.dropout = percent_dropout\n",
    "        self.drop_out = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(300, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.LSTM2 = nn.LSTM(300, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.LSTM3 = nn.LSTM(300, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        if self.LSTM.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.bn = nn.BatchNorm1d(self.hidden_size * self.num_directions)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, c_0\n",
    "    \n",
    "    def forward(self, sentence, mask, lengths):\n",
    "        sort_original = sorted(range(len(lengths)), key=lambda sentence: -lengths[sentence])\n",
    "        unsort_to_original = sorted(range(len(lengths)), key=lambda sentence: sort_original[sentence])\n",
    "        \n",
    "        sentence = sentence[sort_original]\n",
    "        _mask = mask[sort_original]\n",
    "        lengths = lengths[sort_original]\n",
    "        batch_size, seq_len = sentence.size()\n",
    "        self.hidden, self.c_0 = self.init_hidden(batch_size)\n",
    "        \n",
    "        # embdddings\n",
    "        embeds = self.embedding(sentence)\n",
    "        embeds = mask*embeds + (1-_mask)*embeds.clone().detach()\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "        # first lstm\n",
    "        lstm_out, (self.hidden_1, self.c_1) = self.LSTM(embeds, (self.hidden, self.c_0))\n",
    "        emb1, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        emb1 = emb1.view(batch_size, -1, 2, self.hidden_size)\n",
    "        emb1 = torch.max(emb1, dim=1)[0]\n",
    "        emb1 = torch.cat([emb1[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "        emb1 = emb1[unsort_to_original]\n",
    "        \n",
    "        out = self.bn(emb1)\n",
    "        \n",
    "#         lstm_out_2, (self.hidden_2, self.c_2) = self.LSTM2(embeds, (self.hidden_1, self.c_1))\n",
    "#         lstm_out_2, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out_2, batch_first=True)\n",
    "#         lstm_out_2 = lstm_out_2.view(batch_size, -1, 2, self.hidden_size)\n",
    "        \n",
    "#         lstm_out_2 = torch.max(lstm_out_2, dim=1)[0]\n",
    "#         lstm_out_2 = torch.cat([lstm_out_2[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "#         lstm_out_2 = lstm_out_2[unsort_to_original]\n",
    "        \n",
    "#         lstm_out_3, (self.hidden_3, self.c_3) = self.LSTM3(embeds, (self.hidden_2, self.c_2))\n",
    "#         lstm_out_3, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out_3, batch_first=True)\n",
    "#         lstm_out_3 = lstm_out_3.view(batch_size, -1, 2, self.hidden_size)\n",
    "#         lstm_out_3 = torch.max(lstm_out_3, dim=1)[0]\n",
    "        \n",
    "#         lstm_out_3 = torch.cat([lstm_out_3[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "#         lstm_out_3 = lstm_out_3[unsort_to_original]\n",
    "#         out = torch.cat([emb1, lstm_out_2, lstm_out_3], dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading multinli data.\n",
      "Writing numeric label.\n",
      "Reading XNLI ZH data.\n"
     ]
    }
   ],
   "source": [
    "# load train and preprocess\n",
    "print (\"Reading {} data.\".format(config.corpus))\n",
    "nli_train, nli_dev, nli_test = read_enli(nli_corpus=config.corpus)\n",
    "print (\"Writing numeric label.\")\n",
    "if config.corpus == \"multinli\":\n",
    "    nli_train, nli_dev, _ = write_numeric_label(nli_train, nli_dev, nli_test, nli_corpus=config.corpus)\n",
    "elif config.corpus == \"snli\":\n",
    "    nli_train, nli_dev, nli_test = write_numeric_label(nli_train, nli_dev, nli_test, nli_corpus=config.corpus)\n",
    "\n",
    "# load val and test and preprocess\n",
    "print (\"Reading XNLI {} data.\".format(config.val_test_lang.upper()))\n",
    "xnli_dev, xnli_test = read_xnli(config.val_test_lang)\n",
    "_, xnli_dev, xnli_test = write_numeric_label(None, xnli_dev, xnli_test, nli_corpus=\"xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_train, all_train_tokens = tokenize_xnli(nli_train, lang=\"en\")\n",
    "# nli_dev = tokenize_enli(nli_dev)\n",
    "xnli_dev, _ = tokenize_xnli(xnli_dev, lang=config.val_test_lang)\n",
    "xnli_test, _ = tokenize_xnli(xnli_test, lang=config.val_test_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset, max_sentence_length, token2id, id2token):\n",
    "        self.sentence1, self.sentence2, self.labels = [*tokenized_dataset[\"sentence1_tokenized\"].values], \\\n",
    "                                                      [*tokenized_dataset[\"sentence2_tokenized\"].values], \\\n",
    "                                                      [*tokenized_dataset[\"gold_label\"].values]\n",
    "        self.max_sentence_length = int(max_sentence_length)\n",
    "        self.token2id, self.id2token = token2id, id2token\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, row):\n",
    "        label = self.labels[row]\n",
    "        sentence1_word_idx, sentence2_word_idx = [], []\n",
    "        sentence1_mask, sentence2_mask = [], []\n",
    "        for word in self.sentence1[row][:self.max_sentence_length]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence1_word_idx.append(self.token2id[word])\n",
    "                sentence1_mask.append(0)\n",
    "            else:\n",
    "                sentence1_word_idx.append(UNK_IDX)\n",
    "                sentence1_mask.append(1)\n",
    "        for word in self.sentence2[row][:self.max_sentence_length]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence2_word_idx.append(self.token2id[word])\n",
    "                sentence2_mask.append(0)\n",
    "            else:\n",
    "                sentence2_word_idx.append(UNK_IDX)\n",
    "                sentence2_mask.append(1)\n",
    "        sentence1_list = [sentence1_word_idx, sentence1_mask, len(sentence1_word_idx)]\n",
    "        sentence2_list = [sentence2_word_idx, sentence2_mask, len(sentence2_word_idx)]\n",
    "        \n",
    "        return sentence1_list + sentence2_list + [label]\n",
    "\n",
    "def nli_collate_func(batch, max_sent_length):\n",
    "    sentence1_data, sentence2_data = [], []\n",
    "    sentence1_mask, sentence2_mask = [], []\n",
    "    s1_lengths, s2_lengths = [], []\n",
    "    labels = []\n",
    "\n",
    "    for datum in batch:\n",
    "        s1_lengths.append(datum[2])\n",
    "        s2_lengths.append(datum[5])\n",
    "        labels.append(datum[6])\n",
    "        sentence1_data_padded = np.pad(np.array(datum[0]), pad_width=((0, config.max_sent_len-datum[2])), mode=\"constant\", constant_values=0)\n",
    "        sentence1_data.append(sentence1_data_padded)\n",
    "        sentence1_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, config.max_sent_len-datum[2])), mode=\"constant\", constant_values=0)\n",
    "        sentence1_mask.append(sentence1_mask_padded)\n",
    "        sentence2_data_padded = np.pad(np.array(datum[3]), pad_width=((0, config.max_sent_len-datum[5])), mode=\"constant\", constant_values=0)\n",
    "        sentence2_data.append(sentence2_data_padded)\n",
    "        sentence2_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, config.max_sent_len-datum[5])), mode=\"constant\", constant_values=0)\n",
    "        sentence2_mask.append(sentence2_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(s1_lengths)[::-1]\n",
    "    sentence1_data = np.array(sentence1_data)[ind_dec_order]\n",
    "    sentence2_data = np.array(sentence2_data)[ind_dec_order]\n",
    "    sentence1_mask = np.array(sentence1_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    sentence2_mask = np.array(sentence2_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    s1_lengths = np.array(s1_lengths)[ind_dec_order]\n",
    "    s2_lengths = np.array(s2_lengths)[ind_dec_order]\n",
    "    labels = np.array(labels)[ind_dec_order]\n",
    "    \n",
    "    s1_list = [torch.from_numpy(sentence1_data), torch.from_numpy(sentence1_mask).float(), s1_lengths]\n",
    "    s2_list = [torch.from_numpy(sentence2_data), torch.from_numpy(sentence2_mask).float(), s2_lengths]\n",
    "        \n",
    "    return [torch.from_numpy(sentence1_data), torch.from_numpy(sentence1_mask).float(), s1_lengths,\n",
    "            torch.from_numpy(sentence2_data), torch.from_numpy(sentence2_mask).float(), s2_lengths,\n",
    "            labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "nli_train_dataset = NLIDataset(nli_train, max_sentence_length=config.max_sent_len, token2id=token2id_mutual, id2token=id2token_mutual)\n",
    "nli_train_loader = torch.utils.data.DataLoader(dataset=nli_train_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)\n",
    "\n",
    "# dev\n",
    "nli_dev_dataset = NLIDataset(xnli_dev, max_sentence_length=config.max_sent_len, token2id=token2id_mutual, id2token=id2token_mutual)\n",
    "nli_dev_loader = torch.utils.data.DataLoader(dataset=nli_dev_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)\n",
    "\n",
    "# test\n",
    "nli_test_dataset = NLIDataset(xnli_test, max_sentence_length=config.max_sent_len, token2id=token2id_mutual, id2token=id2token_mutual)\n",
    "nli_test_loader = torch.utils.data.DataLoader(dataset=nli_test_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Layers(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, hidden_size_2, percent_dropout,\n",
    "                 interaction_type=\"concat\", classes=3, input_size=300):\n",
    "        \n",
    "        super(Linear_Layers, self).__init__()\n",
    "        self.interaction = interaction_type\n",
    "        self.num_classes = classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.percent_dropout = percent_dropout\n",
    "        self.num_classes = classes\n",
    "        \n",
    "        if self.interaction == \"concat\":\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(4 * self.hidden_size, self.hidden_size_2),\n",
    "                nn.LeakyReLU(0),\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(self.hidden_size_2, self.num_classes))\n",
    "        else:\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(2 * self.hidden_size, self.hidden_size_2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=self.percent_dropout),\n",
    "                nn.Linear(self.hidden_size_2, int(self.hidden_size_2/2)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(int(self.hidden_size_2/2), self.num_classes))\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                nn.init.uniform_(module.bias)\n",
    "\n",
    "    def forward(self, lstm_out_1, lstm_out_2):\n",
    "        if self.interaction == \"concat\":\n",
    "            hidden = torch.cat([lstm_out_1, lstm_out_2, torch.abs(lstm_out_1 - lstm_out_2), \n",
    "                                torch.mul(lstm_out_1, lstm_out_2)], dim=1)\n",
    "        elif self.interaction == \"mul\":\n",
    "            hidden = lstm_out_1*lstm_out_2\n",
    "        elif self.interaction == \"subtract\":\n",
    "            hidden = lstm_out_1-lstm_out_2\n",
    "        hidden = hidden.view(hidden.size(0),-1) \n",
    "        out = self.mlp(hidden)\n",
    "        out = F.log_softmax(out, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(RNN, Linear_Classifier, DataLoader, criterion):\n",
    "\n",
    "    RNN.eval()\n",
    "    Linear_Classifier.eval()\n",
    "    test_loss = 0\n",
    "    label_list = []\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sentence1, s1_original, sentence1_lengths, \n",
    "                    sentence2, s2_original, sentence2_lengths, labels)\\\n",
    "                    in enumerate(DataLoader):\n",
    "\n",
    "            sentence1, s1_original = sentence1.to(device), s1_original.to(device),  \n",
    "            sentence2, s2_original = sentence2.to(device), s2_original.to(device),\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "            output_s1 = RNN(sentence1, s1_original, sentence1_lengths)\n",
    "            output_s2 = RNN(sentence2, s2_original, sentence2_lengths)\n",
    "            out = Linear_Classifier(output_s1, output_s2)\n",
    "            loss = criterion(out, labels)\n",
    "            test_loss += loss.item()/len(DataLoader.dataset)\n",
    "            output_list.append(out)\n",
    "            label_list.append(labels)\n",
    "            \n",
    "    return test_loss, torch.cat(output_list, dim=0), torch.cat(label_list, dim=0)\n",
    "\n",
    "def accuracy(RNN, Linear_Classifier, DataLoader, criterion):\n",
    "    \n",
    "    _, predicted, true_labels = test(RNN = RNN,  Linear_Classifier = Linear_Classifier,\n",
    "                                     DataLoader = DataLoader, criterion = criterion)\n",
    "\n",
    "    predicted = predicted.max(1)[1]\n",
    "    return 100 * predicted.eq(true_labels.data.view_as(predicted)).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = config.max_vocab_size\n",
    "num_classes = 3\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "lstm_hidden_size = 512\n",
    "classifier_hidden_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>language</th>\n",
       "      <th>match</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_tokenized</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34860</th>\n",
       "      <td>[neutral, contradiction, neutral, neutral, neu...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>1</td>\n",
       "      <td>zh</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>他说，妈妈，我回来了。</td>\n",
       "      <td>[他.zh, 说.zh, 妈妈.zh, 我.zh, 回来.zh, 了.zh]</td>\n",
       "      <td>校车把他放下后，他立即给他妈妈打了电话。</td>\n",
       "      <td>[校车.zh, 车把.zh, 他.zh, 放下.zh, 后.zh, 他.zh, 立即.zh,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34861</th>\n",
       "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>2</td>\n",
       "      <td>zh</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>他说，妈妈，我回来了。</td>\n",
       "      <td>[他.zh, 说.zh, 妈妈.zh, 我.zh, 回来.zh, 了.zh]</td>\n",
       "      <td>他没说一句话。</td>\n",
       "      <td>[他.zh, 没.zh, 说.zh, 一句.zh, 话.zh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34862</th>\n",
       "      <td>[entailment, entailment, neutral, entailment, ...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>0</td>\n",
       "      <td>zh</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>他说，妈妈，我回来了。</td>\n",
       "      <td>[他.zh, 说.zh, 妈妈.zh, 我.zh, 回来.zh, 了.zh]</td>\n",
       "      <td>他告诉他的妈妈他已经回到家了。</td>\n",
       "      <td>[他.zh, 告诉.zh, 他.zh, 的.zh, 妈妈.zh, 他.zh, 已经.zh, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        annotator_labels       genre  \\\n",
       "34860  [neutral, contradiction, neutral, neutral, neu...  facetoface   \n",
       "34861  [contradiction, contradiction, contradiction, ...  facetoface   \n",
       "34862  [entailment, entailment, neutral, entailment, ...  facetoface   \n",
       "\n",
       "       gold_label language match  pairID  promptID    sentence1  \\\n",
       "34860           1       zh  True       1         1  他说，妈妈，我回来了。   \n",
       "34861           2       zh  True       2         1  他说，妈妈，我回来了。   \n",
       "34862           0       zh  True       3         1  他说，妈妈，我回来了。   \n",
       "\n",
       "                          sentence1_tokenized             sentence2  \\\n",
       "34860  [他.zh, 说.zh, 妈妈.zh, 我.zh, 回来.zh, 了.zh]  校车把他放下后，他立即给他妈妈打了电话。   \n",
       "34861  [他.zh, 说.zh, 妈妈.zh, 我.zh, 回来.zh, 了.zh]               他没说一句话。   \n",
       "34862  [他.zh, 说.zh, 妈妈.zh, 我.zh, 回来.zh, 了.zh]       他告诉他的妈妈他已经回到家了。   \n",
       "\n",
       "                                     sentence2_tokenized  \n",
       "34860  [校车.zh, 车把.zh, 他.zh, 放下.zh, 后.zh, 他.zh, 立即.zh,...  \n",
       "34861                    [他.zh, 没.zh, 说.zh, 一句.zh, 话.zh]  \n",
       "34862  [他.zh, 告诉.zh, 他.zh, 的.zh, 妈妈.zh, 他.zh, 已经.zh, ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli_dev.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ZH Validation Accuracy = 54.85944151878357\n",
      "\n",
      "ZH Validation Accuracy = 55.38152456283569\n",
      "\n",
      "ZH Validation Accuracy = 54.457831382751465\n",
      "\n",
      "ZH Validation Accuracy = 54.85944151878357\n",
      "\n",
      "ZH Validation Accuracy = 55.22088408470154\n",
      "\n",
      "ZH Validation Accuracy = 55.18072247505188\n",
      "\n",
      "ZH Validation Accuracy = 55.42168617248535\n",
      "\n",
      "ZH Validation Accuracy = 55.301207304000854\n",
      "\n",
      "ZH Validation Accuracy = 55.22088408470154\n",
      "\n",
      "ZH Validation Accuracy = 54.13654446601868\n",
      "\n",
      "ZH Validation Accuracy = 54.57831621170044\n",
      "\n",
      "ZH Validation Accuracy = 53.97590398788452\n",
      "\n",
      "ZH Validation Accuracy = 54.13654446601868\n",
      "\n",
      "ZH Validation Accuracy = 55.020081996917725\n",
      "\n",
      "ZH Validation Accuracy = 54.176706075668335\n",
      "\n",
      "ZH Validation Accuracy = 54.29719090461731\n",
      "\n",
      "ZH Validation Accuracy = 54.57831621170044\n",
      "\n",
      "ZH Validation Accuracy = 54.377514123916626\n",
      "\n",
      "ZH Validation Accuracy = 54.176706075668335\n",
      "\n",
      "ZH Validation Accuracy = 53.97590398788452\n",
      "\n",
      "ZH Validation Accuracy = 53.65461707115173\n",
      "\n",
      "ZH Validation Accuracy = 53.77510190010071\n",
      "\n",
      "ZH Validation Accuracy = 55.50200939178467\n",
      "\n",
      "ZH Validation Accuracy = 54.979920387268066\n",
      "\n",
      "ZH Validation Accuracy = 55.74297308921814\n",
      "\n",
      "ZH Validation Accuracy = 55.542171001434326\n",
      "\n",
      "ZH Validation Accuracy = 55.70281147956848\n",
      "\n",
      "ZH Validation Accuracy = 54.89959716796875\n",
      "\n",
      "ZH Validation Accuracy = 54.89959716796875\n",
      "\n",
      "ZH Validation Accuracy = 55.582332611083984\n",
      "\n",
      "ZH Validation Accuracy = 54.738956689834595\n",
      "\n",
      "ZH Validation Accuracy = 54.65863347053528\n",
      "\n",
      "ZH Validation Accuracy = 54.85944151878357\n",
      "\n",
      "ZH Validation Accuracy = 54.13654446601868\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unexpected EOF. The file might be corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-bb5b2f5d981f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_{}_epoch_{}\".format(config.val_test_lang,\n\u001b[0;32m----> 9\u001b[0;31m                                                                         config.val_test_lang.upper(), epoch)))\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 128,\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF. The file might be corrupted."
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "for i in range(int(1e3)):\n",
    "    time.sleep(5)\n",
    "    LSTM_trg = biLSTM(hidden_size=config.hidden_dim, embedding_weights=weights_init, num_layers=1, percent_dropout = config.dropout, \n",
    "                 vocab_size=weights_init.shape[0], interaction_type=\"concat\", input_size=300, src_trg=\"src\").to(device)\n",
    "\n",
    "    LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_{}_epoch_{}\".format(config.val_test_lang,\n",
    "                                                                        config.val_test_lang.upper(), epoch)))\n",
    "\n",
    "    linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 128,\n",
    "                                 percent_dropout = 0.1, interaction_type=\"concat\", \n",
    "                                 classes=3, input_size=300).to(device)\n",
    "\n",
    "    linear_model.load_state_dict(torch.load(\"best_linear_eng_mnli_{}_{}\".format(3, config.val_test_lang)))\n",
    "    val_acc = accuracy(LSTM_trg, linear_model, nli_dev_loader, nn.NLLLoss(reduction='sum'))\n",
    "    print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ES Validation Accuracy = 63.21285367012024\n",
      "\n",
      "ES Validation Accuracy = 62.97188997268677\n",
      "\n",
      "ES Validation Accuracy = 62.69076466560364\n",
      "\n",
      "ES Validation Accuracy = 62.36947774887085\n",
      "\n",
      "ES Validation Accuracy = 62.85140514373779\n",
      "\n",
      "ES Validation Accuracy = 62.97188997268677\n",
      "\n",
      "ES Validation Accuracy = 62.93172836303711\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.489962577819824\n",
      "\n",
      "ES Validation Accuracy = 62.28916049003601\n",
      "\n",
      "ES Validation Accuracy = 63.092368841171265\n",
      "\n",
      "ES Validation Accuracy = 62.69076466560364\n",
      "\n",
      "ES Validation Accuracy = 63.13253045082092\n",
      "\n",
      "ES Validation Accuracy = 62.65060305595398\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 62.811243534088135\n",
      "\n",
      "ES Validation Accuracy = 63.052207231521606\n",
      "\n",
      "ES Validation Accuracy = 63.052207231521606\n",
      "\n",
      "ES Validation Accuracy = 62.85140514373779\n",
      "\n",
      "ES Validation Accuracy = 63.092368841171265\n",
      "\n",
      "ES Validation Accuracy = 62.771087884902954\n",
      "\n",
      "ES Validation Accuracy = 62.97188997268677\n",
      "\n",
      "ES Validation Accuracy = 62.730926275253296\n",
      "\n",
      "ES Validation Accuracy = 62.89156675338745\n",
      "\n",
      "ES Validation Accuracy = 62.730926275253296\n",
      "\n",
      "ES Validation Accuracy = 63.13253045082092\n",
      "\n",
      "ES Validation Accuracy = 62.97188997268677\n",
      "\n",
      "ES Validation Accuracy = 62.69076466560364\n",
      "\n",
      "ES Validation Accuracy = 62.771087884902954\n",
      "\n",
      "ES Validation Accuracy = 62.811243534088135\n",
      "\n",
      "ES Validation Accuracy = 63.012051582336426\n",
      "\n",
      "ES Validation Accuracy = 63.13253045082092\n",
      "\n",
      "ES Validation Accuracy = 63.092368841171265\n",
      "\n",
      "ES Validation Accuracy = 63.373494148254395\n",
      "\n",
      "ES Validation Accuracy = 63.2530152797699\n",
      "\n",
      "ES Validation Accuracy = 62.85140514373779\n",
      "\n",
      "ES Validation Accuracy = 63.49397897720337\n",
      "\n",
      "ES Validation Accuracy = 63.052207231521606\n",
      "\n",
      "ES Validation Accuracy = 63.092368841171265\n",
      "\n",
      "ES Validation Accuracy = 63.2530152797699\n",
      "\n",
      "ES Validation Accuracy = 63.373494148254395\n",
      "\n",
      "ES Validation Accuracy = 63.373494148254395\n",
      "\n",
      "ES Validation Accuracy = 62.489962577819824\n",
      "\n",
      "ES Validation Accuracy = 62.08835244178772\n",
      "\n",
      "ES Validation Accuracy = 62.61044144630432\n",
      "\n",
      "ES Validation Accuracy = 62.36947774887085\n",
      "\n",
      "ES Validation Accuracy = 62.28916049003601\n",
      "\n",
      "ES Validation Accuracy = 62.730926275253296\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unexpected EOF. The file might be corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f24728c833cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_{}_epoch_{}\".format(config.val_test_lang,\n\u001b[0;32m----> 9\u001b[0;31m                                                                         config.val_test_lang.upper(), epoch)))\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 128,\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF. The file might be corrupted."
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "for i in range(int(1e3)):\n",
    "    time.sleep(5)\n",
    "    LSTM_trg = biLSTM(hidden_size=config.hidden_dim, embedding_weights=weights_init, num_layers=1, percent_dropout = config.dropout, \n",
    "                 vocab_size=weights_init.shape[0], interaction_type=\"concat\", input_size=300, src_trg=\"src\").to(device)\n",
    "\n",
    "    LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_{}_epoch_{}\".format(config.val_test_lang,\n",
    "                                                                        config.val_test_lang.upper(), epoch)))\n",
    "\n",
    "    linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 128,\n",
    "                                 percent_dropout = 0.1, interaction_type=\"concat\", \n",
    "                                 classes=3, input_size=300).to(device)\n",
    "\n",
    "    linear_model.load_state_dict(torch.load(\"best_linear_eng_mnli_{}_{}\".format(2, config.val_test_lang)))\n",
    "    val_acc = accuracy(LSTM_trg, linear_model, nli_dev_loader, nn.NLLLoss(reduction='sum'))\n",
    "    print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), val_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ES Validation Accuracy = 62.32931613922119\n",
      "\n",
      "ES Validation Accuracy = 62.24899888038635\n",
      "\n",
      "ES Validation Accuracy = 62.168675661087036\n",
      "\n",
      "ES Validation Accuracy = 63.012051582336426\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 63.7751042842865\n",
      "\n",
      "ES Validation Accuracy = 63.17269206047058\n",
      "\n",
      "ES Validation Accuracy = 63.2530152797699\n",
      "\n",
      "ES Validation Accuracy = 63.052207231521606\n",
      "\n",
      "ES Validation Accuracy = 62.771087884902954\n",
      "\n",
      "ES Validation Accuracy = 63.17269206047058\n",
      "\n",
      "ES Validation Accuracy = 62.730926275253296\n",
      "\n",
      "ES Validation Accuracy = 63.092368841171265\n",
      "\n",
      "ES Validation Accuracy = 62.208837270736694\n",
      "\n",
      "ES Validation Accuracy = 62.771087884902954\n",
      "\n",
      "ES Validation Accuracy = 62.489962577819824\n",
      "\n",
      "ES Validation Accuracy = 62.85140514373779\n",
      "\n",
      "ES Validation Accuracy = 62.32931613922119\n",
      "\n",
      "ES Validation Accuracy = 62.449800968170166\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.771087884902954\n",
      "\n",
      "ES Validation Accuracy = 62.449800968170166\n",
      "\n",
      "ES Validation Accuracy = 62.28916049003601\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.32931613922119\n",
      "\n",
      "ES Validation Accuracy = 63.052207231521606\n",
      "\n",
      "ES Validation Accuracy = 62.69076466560364\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.12851405143738\n",
      "\n",
      "ES Validation Accuracy = 63.012051582336426\n",
      "\n",
      "ES Validation Accuracy = 63.45381736755371\n",
      "\n",
      "ES Validation Accuracy = 63.41365575790405\n",
      "\n",
      "ES Validation Accuracy = 63.654619455337524\n",
      "\n",
      "ES Validation Accuracy = 62.97188997268677\n",
      "\n",
      "ES Validation Accuracy = 63.092368841171265\n",
      "\n",
      "ES Validation Accuracy = 63.53414058685303\n",
      "\n",
      "ES Validation Accuracy = 62.489962577819824\n",
      "\n",
      "ES Validation Accuracy = 63.21285367012024\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 62.53012418746948\n",
      "\n",
      "ES Validation Accuracy = 62.771087884902954\n",
      "\n",
      "ES Validation Accuracy = 62.65060305595398\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unexpected EOF. The file might be corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-bb5b2f5d981f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_{}_epoch_{}\".format(config.val_test_lang,\n\u001b[0;32m----> 9\u001b[0;31m                                                                         config.val_test_lang.upper(), epoch)))\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 128,\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF. The file might be corrupted."
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "for i in range(int(1e3)):\n",
    "    time.sleep(5)\n",
    "    LSTM_trg = biLSTM(hidden_size=config.hidden_dim, embedding_weights=weights_init, num_layers=1, percent_dropout = config.dropout, \n",
    "                 vocab_size=weights_init.shape[0], interaction_type=\"concat\", input_size=300, src_trg=\"src\").to(device)\n",
    "\n",
    "    LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_{}_epoch_{}\".format(config.val_test_lang,\n",
    "                                                                        config.val_test_lang.upper(), epoch)))\n",
    "\n",
    "    linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 128,\n",
    "                                 percent_dropout = 0.1, interaction_type=\"concat\", \n",
    "                                 classes=3, input_size=300).to(device)\n",
    "\n",
    "    linear_model.load_state_dict(torch.load(\"best_linear_eng_mnli_{}_{}\".format(3, config.val_test_lang)))\n",
    "    val_acc = accuracy(LSTM_trg, linear_model, nli_dev_loader, nn.NLLLoss(reduction='sum'))\n",
    "    print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ES Validation Accuracy = 62.811243534088135\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.69076466560364\n",
      "\n",
      "ES Validation Accuracy = 62.89156675338745\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 62.811243534088135\n",
      "\n",
      "ES Validation Accuracy = 62.69076466560364\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.61044144630432\n",
      "\n",
      "ES Validation Accuracy = 63.012051582336426\n",
      "\n",
      "ES Validation Accuracy = 62.36947774887085\n",
      "\n",
      "ES Validation Accuracy = 61.927711963653564\n",
      "\n",
      "ES Validation Accuracy = 62.208837270736694\n",
      "\n",
      "ES Validation Accuracy = 62.811243534088135\n",
      "\n",
      "ES Validation Accuracy = 62.32931613922119\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 62.489962577819824\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.65060305595398\n",
      "\n",
      "ES Validation Accuracy = 62.32931613922119\n",
      "\n",
      "ES Validation Accuracy = 62.08835244178772\n",
      "\n",
      "ES Validation Accuracy = 61.68674826622009\n",
      "\n",
      "ES Validation Accuracy = 62.04819679260254\n",
      "\n",
      "ES Validation Accuracy = 62.489962577819824\n",
      "\n",
      "ES Validation Accuracy = 62.97188997268677\n",
      "\n",
      "ES Validation Accuracy = 62.208837270736694\n",
      "\n",
      "ES Validation Accuracy = 62.69076466560364\n",
      "\n",
      "ES Validation Accuracy = 62.97188997268677\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 62.32931613922119\n",
      "\n",
      "ES Validation Accuracy = 62.36947774887085\n",
      "\n",
      "ES Validation Accuracy = 62.28916049003601\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 62.449800968170166\n",
      "\n",
      "ES Validation Accuracy = 62.85140514373779\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.40963935852051\n",
      "\n",
      "ES Validation Accuracy = 62.61044144630432\n",
      "\n",
      "ES Validation Accuracy = 62.36947774887085\n",
      "\n",
      "ES Validation Accuracy = 62.489962577819824\n",
      "\n",
      "ES Validation Accuracy = 62.89156675338745\n",
      "\n",
      "ES Validation Accuracy = 62.28916049003601\n",
      "\n",
      "ES Validation Accuracy = 62.85140514373779\n",
      "\n",
      "ES Validation Accuracy = 62.57027983665466\n",
      "\n",
      "ES Validation Accuracy = 62.730926275253296\n",
      "\n",
      "ES Validation Accuracy = 62.489962577819824\n",
      "\n",
      "ES Validation Accuracy = 61.96787357330322\n",
      "\n",
      "ES Validation Accuracy = 62.65060305595398\n",
      "\n",
      "ES Validation Accuracy = 61.96787357330322\n",
      "\n",
      "ES Validation Accuracy = 62.53012418746948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-bb5b2f5d981f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     LSTM_trg = biLSTM(hidden_size=config.hidden_dim, embedding_weights=weights_init, num_layers=1, percent_dropout = config.dropout, \n\u001b[1;32m      6\u001b[0m                  vocab_size=weights_init.shape[0], interaction_type=\"concat\", input_size=300, src_trg=\"src\").to(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "for i in range(int(1e3)):\n",
    "    time.sleep(5)\n",
    "    LSTM_trg = biLSTM(hidden_size=config.hidden_dim, embedding_weights=weights_init, num_layers=1, percent_dropout = config.dropout, \n",
    "                 vocab_size=weights_init.shape[0], interaction_type=\"concat\", input_size=300, src_trg=\"src\").to(device)\n",
    "\n",
    "    LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_{}_epoch_{}\".format(config.val_test_lang,\n",
    "                                                                        config.val_test_lang.upper(), epoch)))\n",
    "\n",
    "    linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 128,\n",
    "                                 percent_dropout = 0.1, interaction_type=\"concat\", \n",
    "                                 classes=3, input_size=300).to(device)\n",
    "\n",
    "    linear_model.load_state_dict(torch.load(\"best_linear_eng_mnli_{}_{}\".format(3, config.val_test_lang)))\n",
    "    val_acc = accuracy(LSTM_trg, linear_model, nli_dev_loader, nn.NLLLoss(reduction='sum'))\n",
    "    print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ES Validation Accuracy = 62.8742516040802\n"
     ]
    }
   ],
   "source": [
    "LSTM_trg = biLSTM(hidden_size=config.hidden_dim, embedding_weights=weights_init, num_layers=1, percent_dropout = config.dropout, \n",
    "                 vocab_size=weights_init.shape[0], interaction_type=\"concat\", input_size=300, src_trg=\"src\").to(device)\n",
    "\n",
    "LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_{}_epoch_{}\".format(config.val_test_lang,\n",
    "                                                                    config.val_test_lang.upper(), epoch)))\n",
    "\n",
    "linear_model = Linear_Layers(hidden_size = classifier_hidden_size, hidden_size_2 = 128,\n",
    "                             percent_dropout = 0.1, interaction_type=\"concat\", \n",
    "                             classes=3, input_size=300).to(device)\n",
    "\n",
    "linear_model.load_state_dict(torch.load(\"best_linear_eng_mnli_{}_{}\".format(2, config.val_test_lang)))\n",
    "test_acc = accuracy(LSTM_trg, linear_model, nli_test_loader, nn.NLLLoss(reduction='sum'))\n",
    "print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
