{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">MultiNLI Training</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "import errno\n",
    "import glob\n",
    "import string\n",
    "import os\n",
    "import jieba\n",
    "import nltk\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Besides the publicly available libraries above, we import our preprocessing functions, models (bidirectional LSTM and linear classifier), and trainer functions. \n",
    "\n",
    "Then we define the pre-set variables using some preprocessing functions:\n",
    "\n",
    "    PAD_IDX: Padding index to use in the vocabulary (here 0)\n",
    "    UNK_IDX: Unkknown index to use in the vocabulary (here 1)\n",
    "    multinli_path: the directory where the MultiNLI dataset is located\n",
    "    align_path: the directory where the aligned vectors are located\n",
    "    multi_path: the directory where the standard multi-lingual vectors are located\n",
    "\n",
    "The code that follows these in the second cell below will specify your device type. (whether you are running on a GPU or a CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from models import *\n",
    "from nli_trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX, UNK_IDX = define_indices()\n",
    "label_dict = define_label_dict()\n",
    "snli_path, align_path, multi_path = define_paths()\n",
    "no_cuda = False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "seed = 1\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SNLIconfig(corpus = \"multinli\", val_test_lang = \"en\", max_sent_len = 50, max_vocab_size = 75000,\n",
    "             epochs = 15, batch_size = 256, embed_dim = 300, hidden_dim = 512, dropout = 0.1, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Tokenize Datasets\n",
    "\n",
    "We use the cell below to read train and dev sets from the MultiNLI corpus. Then we write a numeric label that our models can recognize. It corresponds to \"entailment\", \"contradiction\" or \"neutral\". \n",
    "\n",
    "The tokenizer function here uses standard nltk tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = read_nli(config.corpus)\n",
    "train, dev, test = write_numeric_label(train, dev, test, nli_corpus=config.corpus)\n",
    "train, all_train_tokens = tokenize_xnli(train, lang=config.val_test_lang)\n",
    "dev, _ = tokenize_xnli(dev, lang=config.val_test_lang)\n",
    "# test, _ = tokenize_xnli(test, lang=config.val_test_lang) # will test on XNLI later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Word Embeddings\n",
    "\n",
    "Here we load the pretrained fastText word embeddings using the preprocessing.load_vectors function.\n",
    "\n",
    "Then we construct id2token list and token2id vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = load_vectors(\"../data/vecs/cc.en.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tok = [x+\".\"+\"en\" for x in [*vecs.keys()]][:config.max_vocab_size]\n",
    "id2tok = [\"<PAD>\", \"<UNK>\"] + id2tok\n",
    "tok2id = build_tok2id(id2tok)\n",
    "vecs = update_single_vocab_keys(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init = init_embedding_weights(vecs, tok2id, id2tok, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Data Loaders\n",
    "\n",
    "We specify training and dev data loaders using the NLIDataset class and nli_collate_func from preprocessing.py. We will later use XNLI English test set as the test data for this model. \n",
    "\n",
    "We use these loaders to pass data into training and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_dataset = NLIDataset(train, max_sentence_length=config.max_sent_len, token2id=tok2id, id2token=id2tok)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)\n",
    "\n",
    "# dev\n",
    "dev_dataset = NLIDataset(dev, max_sentence_length=config.max_sent_len, token2id=tok2id, id2token=id2tok)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models on MultiNLI\n",
    "\n",
    "Now we specify our models and train them on MultiNLI training data. At the end of each epoch, we check development (dev) set accuracy using the MultiNLI matched dev dataset. Here is a brief definition of the models & functions we use:\n",
    "\n",
    "    biLSTM: LSTM network, bidirectional by default. Imported from models.py. Takes as input a sentence (premise or hypothesis), encodes it into a fixed-length vector.\n",
    "    \n",
    "    Linear_Layers: linear classifier network from models.py. Takes as input the vector representations of both premise and hypothesis and generates log-likelihood scores for each entailment category (\"entailment\", \"contradiction\", \"neutral\")\n",
    "    \n",
    "    train_: Trainer function for NLI from nli_trainer.py.\n",
    "    \n",
    "    accuracy: Computes accuracy on dev or test set using trained LSTM and linear models. \n",
    "    \n",
    "You can go into each .py file to learn more about the functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      " biLSTM(\n",
      "  (embedding): Embedding(75002, 300)\n",
      "  (drop_out): Dropout(p=0.1)\n",
      "  (LSTM): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      ")\n",
      "Classifier:\n",
      " Linear_Layers(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.1)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.1)\n",
      "    (6): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "epoch = 0\n",
      "Train Epoch: 0 [389120/392702 (99%)]\tLoss: 0.880285\n",
      "EN Validation Accuracy = 58.634740114212036\n",
      "\n",
      "epoch = 1\n",
      "Train Epoch: 1 [389120/392702 (99%)]\tLoss: 0.844064\n",
      "EN Validation Accuracy = 62.221091985702515\n",
      "\n",
      "epoch = 2\n",
      "Train Epoch: 2 [389120/392702 (99%)]\tLoss: 0.791069\n",
      "EN Validation Accuracy = 63.9123797416687\n",
      "\n",
      "epoch = 3\n",
      "Train Epoch: 3 [389120/392702 (99%)]\tLoss: 0.765901\n",
      "EN Validation Accuracy = 64.23841118812561\n",
      "\n",
      "epoch = 4\n",
      "Train Epoch: 4 [389120/392702 (99%)]\tLoss: 0.753534\n",
      "EN Validation Accuracy = 65.05349278450012\n",
      "\n",
      "epoch = 5\n",
      "Train Epoch: 5 [389120/392702 (99%)]\tLoss: 0.733040\n",
      "EN Validation Accuracy = 66.21497869491577\n",
      "\n",
      "epoch = 6\n",
      "Train Epoch: 6 [389120/392702 (99%)]\tLoss: 0.736044\n",
      "EN Validation Accuracy = 66.29648804664612\n",
      "\n",
      "epoch = 7\n",
      "Train Epoch: 7 [389120/392702 (99%)]\tLoss: 0.688341\n",
      "EN Validation Accuracy = 66.75496697425842\n",
      "\n",
      "epoch = 8\n",
      "Train Epoch: 8 [389120/392702 (99%)]\tLoss: 0.664603\n",
      "EN Validation Accuracy = 67.12175011634827\n",
      "\n",
      "epoch = 9\n",
      "Train Epoch: 9 [389120/392702 (99%)]\tLoss: 0.668058\r"
     ]
    }
   ],
   "source": [
    "LSTM = biLSTM(config.hidden_dim, weights_init, config.dropout, config.max_vocab_size,\n",
    "              num_layers=1, input_size=300).to(device)\n",
    "\n",
    "linear_ = Linear_Layers(hidden_size = 1024, hidden_size_2 = 128, percent_dropout = config.dropout,\n",
    "                        classes=3, input_size=config.embed_dim).to(device)\n",
    "\n",
    "print (\"Encoder:\\n\", LSTM)\n",
    "print (\"Classifier:\\n\", linear_)\n",
    "\n",
    "validation_accuracy = [0]\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + config.epochs):\n",
    "    print (\"\\nepoch = \"+str(epoch))\n",
    "    loss_train = train_(LSTM, linear_, DataLoader = train_loader,\n",
    "                       criterion = nn.NLLLoss(),\n",
    "                       optimizer = torch.optim.Adam(list(LSTM.parameters()) + list(linear_.parameters()), \n",
    "                                                   lr=1e-3),\n",
    "                       epoch = epoch)\n",
    "    \n",
    "    val_acc = accuracy(LSTM, linear_, dev_loader, nn.NLLLoss(reduction='sum'))\n",
    "    if val_acc <= validation_accuracy[-1]:\n",
    "        break\n",
    "        \n",
    "    print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), val_acc))\n",
    "    validation_accuracy.append(val_acc)\n",
    "    torch.save(LSTM.state_dict(), \"best_encoder_eng_snli_{}_{}\".format(epoch, \"EN\"))\n",
    "    torch.save(linear_.state_dict(), \"best_linear_eng_snli_{}_{}\".format(epoch, \"EN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
