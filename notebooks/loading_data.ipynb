{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Reading Data</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import csv\n",
    "import string\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"ar\", \"bg\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hi\", \"ru\", \"th\", \"tr\", \"vi\", \"zh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(\" \")\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vectors for ar\n",
      "loading vectors for bg\n",
      "loading vectors for de\n",
      "loading vectors for el\n",
      "loading vectors for en\n",
      "loading vectors for es\n",
      "loading vectors for fr\n",
      "loading vectors for hi\n",
      "loading vectors for ru\n",
      "loading vectors for th\n",
      "loading vectors for tr\n",
      "loading vectors for vi\n",
      "loading vectors for zh\n"
     ]
    }
   ],
   "source": [
    "vector_path = \"../../data/aligned_embeddings\"\n",
    "for x in langs:\n",
    "    print (\"loading vectors for\", x)\n",
    "    fname = \"{}/wiki.{}.align.vec\".format(vector_path, x)\n",
    "    language_dict[x][\"vectors\"] = load_vectors(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load XNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "xnli_dev = pd.read_csv(\"../../data/XNLI/xnli.dev.tsv\", sep=\"\\t\")\n",
    "xnli_test = pd.read_csv(\"../../data/XNLI/xnli.test.tsv\", sep=\"\\t\")\n",
    "mnli_train = pd.read_json(\"../../data/MultiNLI/multinli_1.0_train.jsonl\", lines=True)\n",
    "mnli_dev = pd.read_json(\"../../data/MultiNLI/multinli_1.0_dev_matched.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ar', 'bg', 'de', 'el', 'en', 'es', 'fr', 'hi', 'ru', 'th', 'tr', 'vi', 'zh'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in langs:\n",
    "    language_dict[x][\"xnli_dev\"] = xnli_dev[xnli_dev[\"language\"]==x]\n",
    "    language_dict[x][\"xnli_test\"] = xnli_test[xnli_test[\"language\"]==x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Language Classes\n",
    "\n",
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, remove_punc=False):\n",
    "    all_s1_tokens = []\n",
    "    all_s2_tokens = []\n",
    "    for s in [\"sentence1\", \"sentence2\"]:\n",
    "        if remove_punc:\n",
    "            punc = [*string.punctuation]\n",
    "            dataset[\"{}_tokenized\".format(s)] = dataset[\"{}_tokenized\".format(s)].\\\n",
    "            apply(lambda x: \"\".join(c for c in x if c not in punc).lower().split(\" \"))\n",
    "        else:\n",
    "            dataset[\"{}_tokenized\".format(s)] = dataset[\"{}_tokenized\".format(s)].\\\n",
    "            apply(lambda x: x.lower().split(\" \"))\n",
    "    dataset[\"sentence1_tokenized\"].apply(lambda x: all_s1_tokens.extend(x))\n",
    "    dataset[\"sentence2_tokenized\"].apply(lambda x: all_s2_tokens.extend(x))\n",
    "    all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    return dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "\n",
    "def tokenize_mnli(dataset, remove_punc=True):\n",
    "    punc = string.punctuation\n",
    "    all_s1_tokens = []\n",
    "    all_s2_tokens = []\n",
    "    for s in [1,2]:\n",
    "        if remove_punc:\n",
    "            dataset[\"sentence{}_tokenized\".format(s)] = dataset[\"sentence{}\".format(s)].\\\n",
    "            apply(lambda x: reg.sub(\"\", x).lower().split(\" \"))\n",
    "        else:\n",
    "            dataset[\"sentence{}_tokenized\".format(s)] = dataset[\"sentence{}\".format(s)].\\\n",
    "            apply(lambda x: (reg.sub(\"\", x) + \" .\").lower().split(\" \"))\n",
    "    print (\"Tokenizing done.\")\n",
    "    dataset[\"sentence1_tokenized\"].apply(lambda x: all_s1_tokens.extend(x))\n",
    "    dataset[\"sentence2_tokenized\"].apply(lambda x: all_s2_tokens.extend(x))\n",
    "    print (\"Token collection done.\")\n",
    "    all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    print (\"Concatenation done.\")\n",
    "    return dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing done.\n",
      "Token collection done.\n",
      "Concatenation done.\n"
     ]
    }
   ],
   "source": [
    "mnli_train_tokenized, all_train_tokens = tokenize_mnli(mnli_train, remove_punc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = [*vocab]\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))\n",
    "    id2token = ['<PAD>', '<UNK>'] + id2token\n",
    "    token2id[\"<PAD>\"] = PAD_IDX\n",
    "    token2id[\"<UNK>\"] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Language Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNLILang:\n",
    "    def __init__(self, name, max_vocab_size):\n",
    "        self.name = name\n",
    "        self.xnli_dev, self.xnli_test = language_dict[self.name][\"xnli_dev\"], language_dict[self.name][\"xnli_test\"] \n",
    "        self.tokenized_dev, self.all_dev_tokens = tokenize_dataset(self.xnli_dev, remove_punc=False)\n",
    "        self.tokenized_test, _ = tokenize_dataset(self.xnli_test, remove_punc=False)\n",
    "        self.token2id, self.id2token = build_vocab(self.all_dev_tokens, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNLILang:\n",
    "    def __init__(self, name, max_vocab_size):\n",
    "        self.name = name\n",
    "        self.tokenized_train_data, self.all_train_tokens = tokenize_mnli(mnli_train)\n",
    "        self.train_tokens = all_train_tokens\n",
    "        self.xnli_dev, self.xnli_test = language_dict[self.name][\"xnli_dev\"], language_dict[self.name][\"xnli_test\"] \n",
    "        self.tokenized_dev, self.all_dev_tokens = tokenize_dataset(self.xnli_dev, remove_punc=False)\n",
    "        self.tokenized_test, _ = tokenize_dataset(self.xnli_test, remove_punc=False)\n",
    "        self.token2id, self.id2token = build_vocab(all_train_tokens, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "tr = XNLILang(\"tr\", max_vocab_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ve', 'anne', ',', 'evdeyim', 'dedi', '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.xnli_dev[\"sentence1_tokenized\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class MNLIDataset(Dataset):\n",
    "    def __init__(self, lang, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "        self.sentence1, self.sentence2, self.labels = lang.tokenized_train_data[\"sentence1_tokenized\"].values, \\\n",
    "                                                      lang.tokenized_train_data[\"sentence2_tokenized\"].values, \\\n",
    "                                                      lang.tokenized_train_data[\"gold_label\"].values\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.token2id, self.id2token = lang.token2id, lang.id2token\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, row):\n",
    "        label = self.labels[row]\n",
    "        sentence1_word_idx, sentence2_word_idx = [], []\n",
    "        sentence1_mask, sentence2_mask = [], []\n",
    "        for word in self.sentence1[row][:self.max_sentence_length]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence1_word_idx.append(self.token2id[word])\n",
    "                sentence1_mask.append(0)\n",
    "            else:\n",
    "                sentence1_word_idx.append(UNK_IDX)\n",
    "                sentence1_mask.append(1)\n",
    "        for word in self.sentence2[row][:self.max_sentence_length]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence2_word_idx.append(self.token2id[word])\n",
    "                sentence2_mask.append(0)\n",
    "            else:\n",
    "                sentence2_word_idx.append(UNK_IDX)\n",
    "                sentence2_mask.append(1)\n",
    "        sentence1_list = [sentence1_word_idx, sentence1_mask, len(sentence1_word_idx)]\n",
    "        sentence2_list = [sentence2_word_idx, sentence2_mask, len(sentence2_word_idx)]\n",
    "        \n",
    "        return sentence1_list + sentence2_list + [label]\n",
    "\n",
    "def mnli_func(batch, max_sent_length):\n",
    "    s1_data, s2_data = [], []\n",
    "    s1_mask, s2_mask = [], []\n",
    "    s1_lengths, s2_lengths = [], []\n",
    "    labels = []\n",
    "\n",
    "    for datum in batch:\n",
    "        s1_lengths.append(datum[2])\n",
    "        s2_lengths.append(datum[5])\n",
    "        labels.append(datum[6])\n",
    "\n",
    "        sentence1_data_padded = np.pad(np.array(datum[0]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])), mode=\"constant\", constant_values=0)\n",
    "        sentence1_data.append(sentence1_data_padded)\n",
    "        sentence1_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])), mode=\"constant\", constant_values=0)\n",
    "        sentence1_mask.append(sentence1_mask_padded)\n",
    "        \n",
    "        sentence2_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])), mode=\"constant\", constant_values=0)\n",
    "        sentence2_data.append(sentence2_data_padded)\n",
    "        sentence2_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])), mode=\"constant\", constant_values=0)\n",
    "        sentence2_mask.append(sentence2_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(s1_lengths)[::-1]\n",
    "    s1_data = np.array(s1_data)[ind_dec_order]\n",
    "    s2_data = np.array(s2_data)[ind_dec_order]\n",
    "    s1_mask = np.array(s1_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    s2_mask = np.array(s2_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    s1_lengths = np.array(s1_lengths)[ind_dec_order]\n",
    "    s2_lengths = np.array(s2_lengths)[ind_dec_order]\n",
    "    labels = np.array(labels)[ind_dec_order]\n",
    "    \n",
    "    s1_list = [torch.from_numpy(s1_data), torch.from_numpy(s1_mask).float(), s1_lengths]\n",
    "    s2_list = [torch.from_numpy(s2_data), torch.from_numpy(s2_mask).float(), s2_lengths]\n",
    "        \n",
    "    return s1_list + s2_list + [torch.from_numpy(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
