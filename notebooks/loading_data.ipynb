{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Reading Data</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import csv\n",
    "import string\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"ar\", \"bg\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hi\", \"ru\", \"th\", \"tr\", \"vi\", \"zh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, \"r\", encoding=\"utf-8\", \n",
    "                  newline=\"\\n\", errors=\"ignore\")\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(\" \")\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vectors for ar\n",
      "loading vectors for bg\n",
      "loading vectors for de\n",
      "loading vectors for el\n",
      "loading vectors for en\n",
      "loading vectors for es\n",
      "loading vectors for fr\n",
      "loading vectors for hi\n",
      "loading vectors for ru\n",
      "loading vectors for th\n",
      "loading vectors for tr\n",
      "loading vectors for vi\n",
      "loading vectors for zh\n"
     ]
    }
   ],
   "source": [
    "vector_path = \"../../data/aligned_embeddings\"\n",
    "for x in langs:\n",
    "    print (\"loading vectors for\", x)\n",
    "    fname = \"{}/wiki.{}.align.vec\".format(vector_path, x)\n",
    "    language_dict[x][\"vectors\"] = load_vectors(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load XNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "xnli_dev = pd.read_csv(\"../../data/XNLI/xnli.dev.tsv\", sep=\"\\t\")\n",
    "xnli_test = pd.read_csv(\"../../data/XNLI/xnli.test.tsv\", sep=\"\\t\")\n",
    "mnli_train = pd.read_json(\"../../data/MultiNLI/multinli_1.0_train.jsonl\", lines=True)\n",
    "mnli_dev = # TOOD\n",
    "mnli_test = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>promptID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>genre</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "      <th>sentence1_tokenized</th>\n",
       "      <th>sentence2_tokenized</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ar</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>وقال، ماما، لقد عدت للمنزل.</td>\n",
       "      <td>اتصل بأمه حالما أوصلته حافلة المدرسية.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>وقال ، ماما ، لقد عدت للمنزل .</td>\n",
       "      <td>اتصل بأمه حالما أوصلته حافلة المدرسية .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ar</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>وقال، ماما، لقد عدت للمنزل.</td>\n",
       "      <td>لم ينطق ببنت شفة.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>وقال ، ماما ، لقد عدت للمنزل .</td>\n",
       "      <td>لم ينطق ببنت شفة .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ar</td>\n",
       "      <td>entailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>وقال، ماما، لقد عدت للمنزل.</td>\n",
       "      <td>أخبر أمه أنه قد عاد للمنزل.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>وقال ، ماما ، لقد عدت للمنزل .</td>\n",
       "      <td>أخبر أمه أنه قد عاد للمنزل .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language     gold_label  sentence1_binary_parse  sentence2_binary_parse  \\\n",
       "0       ar        neutral                     NaN                     NaN   \n",
       "1       ar  contradiction                     NaN                     NaN   \n",
       "2       ar     entailment                     NaN                     NaN   \n",
       "\n",
       "   sentence1_parse  sentence2_parse                    sentence1  \\\n",
       "0              NaN              NaN  وقال، ماما، لقد عدت للمنزل.   \n",
       "1              NaN              NaN  وقال، ماما، لقد عدت للمنزل.   \n",
       "2              NaN              NaN  وقال، ماما، لقد عدت للمنزل.   \n",
       "\n",
       "                                sentence2  promptID  pairID       genre  \\\n",
       "0  اتصل بأمه حالما أوصلته حافلة المدرسية.         1       1  facetoface   \n",
       "1                       لم ينطق ببنت شفة.         1       2  facetoface   \n",
       "2             أخبر أمه أنه قد عاد للمنزل.         1       3  facetoface   \n",
       "\n",
       "          label1         label2         label3         label4         label5  \\\n",
       "0        neutral  contradiction        neutral        neutral        neutral   \n",
       "1  contradiction  contradiction  contradiction  contradiction  contradiction   \n",
       "2     entailment     entailment        neutral     entailment     entailment   \n",
       "\n",
       "              sentence1_tokenized                      sentence2_tokenized  \\\n",
       "0  وقال ، ماما ، لقد عدت للمنزل .  اتصل بأمه حالما أوصلته حافلة المدرسية .   \n",
       "1  وقال ، ماما ، لقد عدت للمنزل .                       لم ينطق ببنت شفة .   \n",
       "2  وقال ، ماما ، لقد عدت للمنزل .             أخبر أمه أنه قد عاد للمنزل .   \n",
       "\n",
       "   match  \n",
       "0   True  \n",
       "1   True  \n",
       "2   True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli_dev.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ar', 'bg', 'de', 'el', 'en', 'es', 'fr', 'hi', 'ru', 'th', 'tr', 'vi', 'zh'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in langs:\n",
    "    language_dict[x][\"xnli_dev\"] = xnli_dev[xnli_dev[\"language\"]==x]\n",
    "    language_dict[x][\"xnli_test\"] = xnli_test[xnli_test[\"language\"]==x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Language Classes\n",
    "\n",
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, remove_punc=False):\n",
    "    all_tokens = []\n",
    "    for s in [\"sentence1\", \"sentence2\"]:\n",
    "        if remove_punc:\n",
    "            punc = [*string.punctuation]\n",
    "            dataset[\"{}_tokenized\".format(s)] = dataset[\"{}_tokenized\".format(s)].\\\n",
    "            apply(lambda x: \"\".join(c for c in x if c not in punc).lower().split(\" \"))\n",
    "        else:\n",
    "            dataset[\"{}_tokenized\".format(s)] = dataset[\"{}_tokenized\".format(s)].\\\n",
    "            apply(lambda x: x.lower().split(\" \"))\n",
    "    all_s1_tokens = functools.reduce(lambda x, y: x + y, [*dataset[\"sentence1_tokenized\"]])\n",
    "    all_s2_tokens = functools.reduce(lambda x, y: x + y, [*dataset[\"sentence2_tokenized\"]])\n",
    "    all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    return dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "\n",
    "def tokenize_mnli(dataset, remove_punc=True):\n",
    "    all_tokens = []\n",
    "    punc = string.punctuation\n",
    "    for s in [1,2]:\n",
    "        if remove_punc:\n",
    "            dataset[\"sentence{}_tokenized\".format(s)] = dataset[\"sentence{}\".format(s)].\\\n",
    "            apply(lambda x: reg.sub(\"\", x).lower().split(\" \"))\n",
    "        else:\n",
    "            dataset[\"sentence{}_tokenized\".format(s)] = dataset[\"sentence{}\".format(s)].\\\n",
    "            apply(lambda x: (reg.sub(\"\", x) + \" .\").lower().split(\" \"))\n",
    "    print (\"Tokenizing done.\")\n",
    "    all_s1_tokens = functools.reduce(lambda x, y: x + y, dataset[\"sentence1_tokenized\"])\n",
    "    all_s2_tokens = functools.reduce(lambda x, y: x + y, dataset[\"sentence2_tokenized\"])\n",
    "    print (\"Token collection done.\")\n",
    "    all_tokens = all_s1_tokens + all_s2_tokens\n",
    "    return dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing done.\n"
     ]
    }
   ],
   "source": [
    "mnli_train_tokenized, all_train_tokens = tokenize_mnli(mnli_train, remove_punc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mnli_train_tokenized, open(\"mnli_train_tokenized.pickle\", \"wb\"))\n",
    "pickle.dump(all_train_tokens, open(\"all_train_tokens_mnli.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(troken_counter.most_common(max_vocab_size))\n",
    "    id2token = [*vocab]\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))\n",
    "    id2token = ['<PAD>', '<UNK>'] + id2token\n",
    "    token2id[\"<PAD>\"] = PAD_IDX\n",
    "    token2id[\"<UNK>\"] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Language Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNLILang:\n",
    "    # all langs besides en\n",
    "    def __init__(self, name, max_vocab_size):\n",
    "        self.name = name\n",
    "        self.vectors = language_dict[self.name][\"vectors\"]\n",
    "        self.xnli_dev, self.xnli_test = language_dict[self.name][\"xnli_dev\"], language_dict[self.name][\"xnli_test\"] \n",
    "        self.tokenized_dev, self.all_dev_tokens = tokenize_dataset(self.xnli_dev)\n",
    "        self.tokenized_test, _ = tokenize_dataset(self.xnli_test)\n",
    "        self.token2id, self.id2token = build_vocab(self.all_dev_tokens, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNLILang:\n",
    "    # en-only\n",
    "    def __init__(self, name, max_vocab_size):\n",
    "        self.name = name\n",
    "        self.vectors = language_dict[self.name][\"vectors\"]\n",
    "        self.train = language_dict[self.name][\"mnli_train_tokenized\"]\n",
    "        self.train_tokens = all_train_tokens\n",
    "        self.xnli_dev, self.xnli_test = language_dict[self.name][\"xnli_dev\"], language_dict[self.name][\"xnli_test\"] \n",
    "        self.tokenized_dev, self.all_dev_tokens = tokenize_dataset(self.xnli_dev)\n",
    "        self.tokenized_test, _ = tokenize_dataset(self.xnli_test)\n",
    "        self.token2id, self.id2token = build_vocab(all_train_tokens, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
