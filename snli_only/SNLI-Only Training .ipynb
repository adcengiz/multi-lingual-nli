{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">SNLI Training</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "import errno\n",
    "import glob\n",
    "import string\n",
    "import os\n",
    "import jieba\n",
    "import nltk\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Besides the publicly available libraries above, we import our preprocessing functions, models (bidirectional LSTM and linear classifier), and trainer functions. \n",
    "\n",
    "Then we define the pre-set variables using some preprocessing functions:\n",
    "\n",
    "    PAD_IDX: Padding index to use in the vocabulary (here 0)\n",
    "    UNK_IDX: Unkknown index to use in the vocabulary (here 1)\n",
    "    snli_path: the directory where the SNLI dataset is located\n",
    "    align_path: the directory where the aligned vectors are located\n",
    "    multi_path: the directory where the standard multi-lingual vectors are located\n",
    "\n",
    "The code that follows these in the second cell below will specify your device type. (whether you are running on a GPU or a CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from models import *\n",
    "from nli_trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX, UNK_IDX = define_indices()\n",
    "label_dict = define_label_dict()\n",
    "snli_path, align_path, multi_path = define_paths()\n",
    "no_cuda = False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "seed = 1\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SNLIconfig(corpus = \"snli\", val_test_lang = \"en\", max_sent_len = 50, max_vocab_size = 100000,\n",
    "             epochs = 15, batch_size = 64, embed_dim = 300, hidden_dim = 512, dropout = 0.1, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Tokenize Datasets\n",
    "\n",
    "We use the cell below to read train, dev, and test sets from the SNLI corpus. Then we write a numeric label that our models can recognize. It corresponds to \"entailment\", \"contradiction\" or \"neutral\". \n",
    "\n",
    "The tokenizer function here uses standard nltk tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = read_nli(config.corpus)\n",
    "train, dev, test = write_numeric_label(train, dev, test, nli_corpus=config.corpus)\n",
    "train, all_train_tokens = tokenize_xnli(train, lang=config.val_test_lang)\n",
    "dev, _ = tokenize_xnli(dev, lang=config.val_test_lang)\n",
    "test, _ = tokenize_xnli(test, lang=config.val_test_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Word Embeddings\n",
    "\n",
    "Here we load the pretrained fastText word embeddings using the preprocessing.load_vectors function.\n",
    "\n",
    "Then we construct id2token list and token2id vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = load_vectors(\"../data/vecs/cc.en.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tok = [x+\".\"+\"en\" for x in [*vecs.keys()]][:config.max_vocab_size]\n",
    "id2tok = [\"<PAD>\", \"<UNK>\"] + id2tok\n",
    "tok2id = build_tok2id(id2tok)\n",
    "vecs = update_single_vocab_keys(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init = init_embedding_weights(vecs, tok2id, id2tok, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Data Loaders\n",
    "\n",
    "We specify training, dev, and test data loaders using the NLIDataset class and nli_collate_func from preprocessing.py. Below we use these loaders to pass data into training and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_dataset = NLIDataset(train, max_sentence_length=config.max_sent_len, token2id=tok2id, id2token=id2tok)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)\n",
    "\n",
    "# dev\n",
    "dev_dataset = NLIDataset(dev, max_sentence_length=config.max_sent_len, token2id=tok2id, id2token=id2tok)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)\n",
    "\n",
    "# test\n",
    "test_dataset = NLIDataset(test, max_sentence_length=config.max_sent_len, token2id=tok2id, id2token=id2tok)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models on SNLI\n",
    "\n",
    "Now we specify our models and train them on SNLI training data. At the end of each epoch, we check development (dev) set accuracy using the SNLI dev dataset. Here is a brief definition of the models & functions we use:\n",
    "\n",
    "    biLSTM: LSTM network, bidirectional by default. Imported from models.py. Takes as input a sentence (premise or hypothesis), encodes it into a fixed-length vector.\n",
    "    \n",
    "    Linear_Layers: linear classifier network from models.py. Takes as input the vector representations of both premise and hypothesis and generates log-likelihood scores for each entailment category (\"entailment\", \"contradiction\", \"neutral\")\n",
    "    \n",
    "    train_: Trainer function for NLI from nli_trainer.py.\n",
    "    \n",
    "    accuracy: Computes accuracy on dev or test set using trained LSTM and linear models. \n",
    "    \n",
    "You can go into each .py file to learn more about the functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      " biLSTM(\n",
      "  (embedding): Embedding(100002, 300)\n",
      "  (drop_out): Dropout(p=0.1)\n",
      "  (LSTM): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      ")\n",
      "Classifier:\n",
      " Linear_Layers(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.1)\n",
      "    (3): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "epoch = 0\n",
      "Train Epoch: 0 [549120/549367 (100%)]\tLoss: 0.793461\n",
      "EN Validation Accuracy = 74.7815489768982\n",
      "\n",
      "epoch = 1\n",
      "Train Epoch: 1 [549120/549367 (100%)]\tLoss: 0.793730\n",
      "EN Validation Accuracy = 77.89067029953003\n",
      "\n",
      "epoch = 2\n",
      "Train Epoch: 2 [549120/549367 (100%)]\tLoss: 0.763852\n",
      "EN Validation Accuracy = 79.45539355278015\n",
      "\n",
      "epoch = 3\n",
      "Train Epoch: 3 [549120/549367 (100%)]\tLoss: 0.650406\n",
      "EN Validation Accuracy = 80.12599349021912\n",
      "\n",
      "epoch = 4\n",
      "Train Epoch: 4 [549120/549367 (100%)]\tLoss: 0.656655\n",
      "EN Validation Accuracy = 80.64417839050293\n",
      "\n",
      "epoch = 5\n",
      "Train Epoch: 5 [549120/549367 (100%)]\tLoss: 0.615389\n",
      "EN Validation Accuracy = 81.09124302864075\n",
      "\n",
      "epoch = 6\n",
      "Train Epoch: 6 [549120/549367 (100%)]\tLoss: 0.569549\r"
     ]
    }
   ],
   "source": [
    "LSTM = biLSTM(config.hidden_dim, weights_init, config.dropout, config.max_vocab_size,\n",
    "              num_layers=1, input_size=300).to(device)\n",
    "\n",
    "linear_ = Linear_Layers(hidden_size = 1024, hidden_size_2 = 128, percent_dropout = config.dropout,\n",
    "                        classes=3, input_size=config.embed_dim).to(device)\n",
    "\n",
    "print (\"Encoder:\\n\", LSTM)\n",
    "print (\"Classifier:\\n\", linear_)\n",
    "\n",
    "validation_accuracy = [0]\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + config.epochs):\n",
    "    print (\"\\nepoch = \"+str(epoch))\n",
    "    loss_train = train_(LSTM, linear_, DataLoader = train_loader,\n",
    "                       criterion = nn.NLLLoss(),\n",
    "                       optimizer = torch.optim.Adam(list(LSTM.parameters()) + list(linear_.parameters()), \n",
    "                                                   lr=config.lr, weight_decay=0),\n",
    "                       epoch = epoch)\n",
    "    \n",
    "    val_acc = accuracy(LSTM, linear_, dev_loader, nn.NLLLoss(reduction='sum'))\n",
    "    if val_acc <= validation_accuracy[-1]:\n",
    "        break\n",
    "        \n",
    "    print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), val_acc))\n",
    "    validation_accuracy.append(val_acc)\n",
    "    torch.save(LSTM.state_dict(), \"best_encoder_eng_snli_{}_{}\".format(epoch, \"EN\"))\n",
    "    torch.save(linear_.state_dict(), \"best_linear_eng_snli_{}_{}\".format(epoch, \"EN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model on SNLI Test Set\n",
    "\n",
    "We pick the model that yields the highest accuracy on the development set, then compute its accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = accuracy(LSTM, linear_, test_loader, nn.NLLLoss(reduction='sum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.62906861305237"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained on the SNLI training set achieves 80.6% accuracy on the SNLI test set. Here is a summary of the stats.\n",
    "\n",
    "    - The model trained on SNLI training data for 6 epochs. \n",
    "    \n",
    "    - Highest validation (dev) accuracy recorded is 81%. \n",
    "    \n",
    "    - The NLL Loss on training set before stopping early was 0.62. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
