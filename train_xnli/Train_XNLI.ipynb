{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">XNLI Training</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "import errno\n",
    "import glob\n",
    "import string\n",
    "import os\n",
    "import jieba\n",
    "import nltk\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Besides the publicly available libraries above, we import our preprocessing functions, models (bidirectional LSTM and linear classifier), discriminator (for the encoder alignment) and aligner (alignment trainer) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from preprocess import *\n",
    "from Discriminator import *\n",
    "from aligner_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX, UNK_IDX = define_indices()\n",
    "label_dict = define_label_dict()\n",
    "snli_path, align_path, multi_path = define_paths()\n",
    "no_cuda = False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "seed = 1\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = XNLIconfig(val_test_lang = \"de\", max_sent_len = 50, max_vocab_size = 100000,\n",
    "                    epochs = 15, batch_size = 256, embed_dim = 300, hidden_dim = 512, dropout = 0.1, lr = 1e-3,\n",
    "                    experiment_lang = \"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align Source and Target Encoders\n",
    "\n",
    "In this section, we load the encoder LSTM that was trained on the [English] MNLI training set and align the target encoder to the English encoder. The process works like this:\n",
    "\n",
    "    I. Load English encoder and fix its parameters.\n",
    "    II. Make a copy of it (with free non-fixed parameters), also dubbed the target encoder.\n",
    "    III. Define a loss function for measuring the alignment degree of source and target sentence embeddings. To see the details of our loss, go to aligner_functions.py>loss_align.\n",
    "    IV. Align the target encoder to the source encoder using this loss function so that it produces close-enough embeddings of target language sentences so that the linear classifier confuses it with English.\n",
    "    V. Here we also use a discriminator model to measure the degree of confusion created by the alignment process.\n",
    "    VI. Load your aligned target encoder and measure XNLI target language dev set accuracy.\n",
    "    VII. Go to step I and reiterate with different parameter sets until you find the optimal model.\n",
    "    VIII. Test your model on XNLI target language test set.\n",
    "\n",
    "### Load Source & Target Vectors\n",
    "\n",
    "Here we first load the source and target language vectors separately. Then we concatenate those into one large embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors for EN.\n",
      "Loading vectors for DE.\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading vectors for EN.\")\n",
    "src_vectors = load_vectors(\"../data/vecs/cc.en.300.vec\")\n",
    "print (\"Loading vectors for {}.\".format(config.experiment_lang.upper()))\n",
    "trg_vectors = load_vectors(\"../data/vecs/cc.de.300.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare mutual vocabulary for English and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tok_src = [x+\".\"+\"en\" for x in [*src_vectors.keys()]][:config.max_vocab_size]\n",
    "id2tok_trg = [x+\".\"+ config.experiment_lang for x in [*trg_vectors.keys()]][:config.max_vocab_size]\n",
    "id2tok_mutual = [\"<PAD>\", \"<UNK>\"] + id2tok_src + id2tok_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_mutual = update_vocab_keys(src_vectors, trg_vectors)\n",
    "tok2id_mutual = build_tok2id(id2tok_mutual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init = init_embedding_weights(vecs_mutual, tok2id_mutual, id2tok_mutual, config.embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read & Tokenize Parallel Corpus\n",
    "\n",
    "For this demo, we use Europarl English-German parallel corpus that holds the one-to-one translation records of the EU parliament discussions. For languages that are not present among the Europarl parallel corpora, such as Arabic and Chinese, we use OPUS (Open Subtitles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en_target, all_en_tokens, all_target_tokens = read_and_tokenize_europarl_data(lang=config.val_test_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a contrastive dataset that is generated by __batch internal shuffling__. We basically take our dataset and shuffle the order of either the source or the target language (or sometimes both). We use this portion to calculate the contrastive part of the loss. For more details, refer to the alignment loss function at ```aligner_functions.py > loss_align```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = create_contrastive_dataset(data_en_target, config.experiment_lang, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\"en\", config.experiment_lang]:\n",
    "    data_en_target[\"len_{}\".format(x)] = data_en_target[x + \"_tokenized\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en_target = data_en_target[(data_en_target[\"len_en\"]>2)&(data_en_target[\"len_{}\".format(config.experiment_lang)]>2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_dataset = AlignDataset(data_en_target, config.max_sent_len, \"en\", config.experiment_lang,\n",
    "                             tok2id_mutual, id2tok_mutual)\n",
    "align_loader = torch.utils.data.DataLoader(dataset=align_dataset, batch_size=config.batch_size,\n",
    "                                           collate_fn=lambda x, max_sentence_length=config.max_sent_len: align_collate_func(x, config.max_sent_len),\n",
    "                                           shuffle=False)\n",
    "\n",
    "c_align_dataset = AlignDataset(c_df, config.max_sent_len, \"en\", config.experiment_lang,\n",
    "                               tok2id_mutual, id2tok_mutual)\n",
    "c_align_loader = torch.utils.data.DataLoader(dataset=c_align_dataset, batch_size=config.batch_size,\n",
    "                                             collate_fn=lambda x, max_sentence_length=config.max_sent_len: align_collate_func(x, config.max_sent_len),\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_epoch = 8\n",
    "LSTM_src_model = biLSTM(config.hidden_dim, weights_init, config.dropout, config.max_vocab_size,\n",
    "                        num_layers=1, input_size=300).to(device)\n",
    "\n",
    "LSTM_src_model.load_state_dict(torch.load(\"best_encoder_eng_mnli_{}_EN\".format(load_epoch)))\n",
    "# fix source encoder parameters\n",
    "for param in LSTM_src_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "LSTM_trg_model = biLSTM(config.hidden_dim, weights_init, config.dropout, config.max_vocab_size,\n",
    "                        num_layers=1, input_size=300).to(device)\n",
    "\n",
    "LSTM_trg_model.load_state_dict(torch.load(\"best_encoder_eng_mnli_{}_EN\".format(load_epoch)))\n",
    "\n",
    "disc = Discriminator(n_langs = 2, dis_layers = 3, dis_hidden_dim = 128, dis_dropout = 0.1, lr_slope=0.005).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder src:\n",
      " biLSTM(\n",
      "  (embedding): Embedding(200002, 300)\n",
      "  (drop_out): Dropout(p=0.1)\n",
      "  (LSTM): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      ")\n",
      "Encoder trg:\n",
      " biLSTM(\n",
      "  (embedding): Embedding(200002, 300)\n",
      "  (drop_out): Dropout(p=0.1)\n",
      "  (LSTM): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      ")\n",
      "Discriminator:\n",
      " Discriminator(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.005)\n",
      "    (2): Dropout(p=0.1)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.005)\n",
      "    (5): Dropout(p=0.1)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.005)\n",
      "    (8): Dropout(p=0.1)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (\"Encoder src:\\n\", LSTM_src_model)\n",
    "print (\"Encoder trg:\\n\", LSTM_trg_model)\n",
    "print (\"Discriminator:\\n\", disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_init = torch.from_numpy(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.epochs):\n",
    "    print (\"\\nepoch = \"+str(epoch))\n",
    "    \n",
    "    loss_train = train(LSTM_s=LSTM_src_model, LSTM_t=LSTM_trg_model, discriminator = disc,\n",
    "                       loader=align_loader, contrastive_loader=c_align_loader,\n",
    "                       optimizer = torch.optim.Adam([*LSTM_src_model.parameters()] + [*LSTM_trg_model.parameters()] + [*disc.parameters()],\n",
    "                                                    lr=config.lr),\n",
    "                       dis_optim = torch.optim.Adam([*disc.parameters()],\n",
    "                                                    lr=config.lr),\n",
    "                       epoch = epoch)\n",
    "        \n",
    "    torch.save(LSTM_trg_model.state_dict(), \"LSTM_en_{}_{}_epoch_{}\".format(config.experiment_lang, config.experiment_lang.upper(), epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Accuracy on XNLI Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nli_trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init = weights_init.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading XNLI DE data.\n"
     ]
    }
   ],
   "source": [
    "# load val and test and preprocess\n",
    "print (\"Reading XNLI {} data.\".format(config.val_test_lang.upper()))\n",
    "xnli_dev, xnli_test = read_xnli(config.experiment_lang)\n",
    "_, xnli_dev, xnli_test = write_numeric_label(None, xnli_dev, xnli_test, nli_corpus=\"xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_dev, _ = tokenize_xnli(xnli_dev, lang=config.val_test_lang)\n",
    "xnli_test, _ = tokenize_xnli(xnli_test, lang=config.val_test_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "nli_dev_dataset = NLIDataset(xnli_dev, max_sentence_length=config.max_sent_len, token2id=tok2id_mutual, id2token=id2tok_mutual)\n",
    "nli_dev_loader = torch.utils.data.DataLoader(dataset=nli_dev_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)\n",
    "\n",
    "# test\n",
    "nli_test_dataset = NLIDataset(xnli_test, max_sentence_length=config.max_sent_len, token2id=tok2id_mutual, id2token=id2tok_mutual)\n",
    "nli_test_loader = torch.utils.data.DataLoader(dataset=nli_test_dataset, batch_size=config.batch_size,\n",
    "                               collate_fn=lambda x, max_sentence_length=config.max_sent_len: nli_collate_func(x, config.max_sent_len),\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DE Validation Accuracy = 55.38152456283569\n"
     ]
    }
   ],
   "source": [
    "LSTM_trg = biLSTM(config.hidden_dim, weights_init, config.dropout, config.max_vocab_size,\n",
    "                        num_layers=1, input_size=300).to(device)\n",
    "epoch = 0\n",
    "LSTM_trg.load_state_dict(torch.load(\"LSTM_en_{}_epoch_{}\".format(config.val_test_lang.upper(), epoch)))\n",
    "\n",
    "linear_model = Linear_Layers(hidden_size = 1024, hidden_size_2 = 128, percent_dropout = config.dropout,\n",
    "                        classes=3, input_size=config.embed_dim).to(device)\n",
    "\n",
    "linear_model.load_state_dict(torch.load(\"best_linear_eng_mnli_{}_{}\".format(8, \"EN\")))\n",
    "val_acc = accuracy(LSTM_trg, linear_model, nli_dev_loader, nn.NLLLoss(reduction='sum'))\n",
    "print (\"\\n{} Validation Accuracy = {}\".format(config.val_test_lang.upper(), val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
